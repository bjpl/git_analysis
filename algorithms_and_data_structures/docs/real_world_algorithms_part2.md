# Real-World Algorithms: Complete Pseudocode Examples (Part 2)

## Environmental Science

### ğŸŒ¡ï¸ Weather Prediction (Numerical Weather Model)
**Purpose**: Forecasts weather using atmospheric physics equations
**Real Usage**: NOAA, European Centre for Medium-Range Weather Forecasts

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Grid-Based Atmospheric Weather Simulation              â•‘
â•‘ Resolution: 10km grid | Forecast: 7 days | Updates: 6-hourly      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  current_state: Grid3D[Atmosphere]  # 3D grid of atmospheric data
  terrain_map: Grid2D[float]         # Elevation data
  time_step: float                   # Simulation time increment (seconds)
  forecast_hours: int                # How far to predict
  
â–¶ OUTPUT:
  forecast: array[Grid3D[Atmosphere]]  # Future atmospheric states
  precipitation: Grid2D[float]         # Rainfall prediction map
  temperature: Grid2D[float]           # Temperature prediction map
  warnings: array[WeatherAlert]        # Severe weather alerts

â–¶ DATA STRUCTURES:
  Atmosphere: {
    temperature: float      # Kelvin
    pressure: float        # Pascals
    humidity: float        # Relative humidity (0-1)
    wind_u: float          # East-west wind component (m/s)
    wind_v: float          # North-south wind component (m/s)
    wind_w: float          # Vertical wind component (m/s)
  }
  
  Grid3D: {
    data: array[x][y][z]   # x=longitude, y=latitude, z=altitude
    resolution: float      # Grid spacing in km
  }

â–¶ ALGORITHM:
FUNCTION predictWeather(current_state, terrain_map, time_step, forecast_hours):
    
    â•â•â•â•â•â• STEP 1: Initialize Model Grid â•â•â•â•â•â•
    nx, ny, nz â† getDimensions(current_state)
    forecast â† []
    state â† current_state.copy()
    
    # Physical constants
    g â† 9.81                    # Gravity (m/sÂ²)
    R â† 287                     # Gas constant for air
    Cp â† 1004                   # Specific heat capacity
    L â† 2.5e6                   # Latent heat of vaporization
    
    total_steps â† (forecast_hours Ã— 3600) / time_step
    
    â•â•â•â•â•â• STEP 2: Time Integration Loop (Finite Difference Method) â•â•â•â•â•â•
    FOR step FROM 0 TO total_steps:
        
        # Create temporary arrays for new values
        new_state â† createGrid3D(nx, ny, nz)
        
        FOR i FROM 1 TO nx-2:      # Exclude boundaries
            FOR j FROM 1 TO ny-2:
                FOR k FROM 1 TO nz-2:
                    
                    # Get current cell and neighbors
                    cell â† state[i][j][k]
                    
                    â”€â”€â”€ SUBSTEP 2.1: Calculate Pressure Gradient Force â”€â”€â”€
                    # Pressure gradient in x-direction
                    dP_dx â† (state[i+1][j][k].pressure - state[i-1][j][k].pressure) / (2 Ã— resolution)
                    # Pressure gradient in y-direction  
                    dP_dy â† (state[i][j+1][k].pressure - state[i][j-1][k].pressure) / (2 Ã— resolution)
                    # Pressure gradient in z-direction
                    dP_dz â† (state[i][j][k+1].pressure - state[i][j][k-1].pressure) / (2 Ã— resolution)
                    
                    â”€â”€â”€ SUBSTEP 2.2: Apply Momentum Equations (Navier-Stokes) â”€â”€â”€
                    # Update wind components
                    new_u â† cell.wind_u - (time_step / cell.pressure) Ã— dP_dx
                    new_v â† cell.wind_v - (time_step / cell.pressure) Ã— dP_dy
                    new_w â† cell.wind_w - (time_step / cell.pressure) Ã— dP_dz - g Ã— time_step
                    
                    # Apply Coriolis force (Earth's rotation effect)
                    latitude â† getLatitude(j)
                    f â† 2 Ã— 7.27e-5 Ã— sin(latitude)  # Coriolis parameter
                    new_u â† new_u + f Ã— cell.wind_v Ã— time_step
                    new_v â† new_v - f Ã— cell.wind_u Ã— time_step
                    
                    â”€â”€â”€ SUBSTEP 2.3: Thermodynamic Equation â”€â”€â”€
                    # Advection of temperature
                    dT_dx â† (state[i+1][j][k].temperature - state[i-1][j][k].temperature) / (2 Ã— resolution)
                    dT_dy â† (state[i][j+1][k].temperature - state[i][j-1][k].temperature) / (2 Ã— resolution)
                    dT_dz â† (state[i][j][k+1].temperature - state[i][j][k-1].temperature) / (2 Ã— resolution)
                    
                    advection â† -(cell.wind_u Ã— dT_dx + cell.wind_v Ã— dT_dy + cell.wind_w Ã— dT_dz)
                    
                    # Adiabatic cooling/heating
                    adiabatic â† -(g / Cp) Ã— cell.wind_w
                    
                    new_temperature â† cell.temperature + time_step Ã— (advection + adiabatic)
                    
                    â”€â”€â”€ SUBSTEP 2.4: Moisture and Cloud Formation â”€â”€â”€
                    # Calculate saturation vapor pressure
                    e_sat â† 611.2 Ã— exp(17.67 Ã— (cell.temperature - 273.15) / (cell.temperature - 29.65))
                    
                    # Current vapor pressure
                    e â† cell.humidity Ã— e_sat
                    
                    # Check for condensation (cloud/rain formation)
                    IF e > e_sat:
                        condensation_rate â† (e - e_sat) / (L Ã— time_step)
                        new_humidity â† e_sat / e_sat  # Saturated at 100%
                        
                        # Release latent heat
                        new_temperature â† new_temperature + (L Ã— condensation_rate / Cp)
                        
                        # Track precipitation
                        IF k == 0:  # Ground level
                            precipitation[i][j] += condensation_rate Ã— time_step
                    ELSE:
                        new_humidity â† cell.humidity
                    
                    â”€â”€â”€ SUBSTEP 2.5: Update State â”€â”€â”€
                    new_state[i][j][k] â† Atmosphere{
                        temperature: new_temperature,
                        pressure: calculatePressure(new_temperature, k, terrain_map[i][j]),
                        humidity: new_humidity,
                        wind_u: new_u,
                        wind_v: new_v,
                        wind_w: new_w
                    }
        
        â•â•â•â•â•â• STEP 3: Apply Boundary Conditions â•â•â•â•â•â•
        applyBoundaryConditions(new_state, terrain_map)
        
        â•â•â•â•â•â• STEP 4: Numerical Stability Check â•â•â•â•â•â•
        IF checkCFL(new_state, time_step, resolution) > 1.0:
            PRINT "Warning: CFL condition violated, reducing time step"
            time_step â† time_step Ã— 0.5
        
        state â† new_state
        
        # Save forecast at 6-hour intervals
        IF step % (6 Ã— 3600 / time_step) == 0:
            forecast.append(state.copy())
    
    â•â•â•â•â•â• STEP 5: Generate Weather Alerts â•â•â•â•â•â•
    warnings â† []
    
    FOR point IN forecast[-1]:  # Check final forecast
        # Severe storm detection
        wind_speed â† sqrt(point.wind_uÂ² + point.wind_vÂ²)
        IF wind_speed > 32:  # Hurricane force
            warnings.append(WeatherAlert{
                type: "HURRICANE",
                location: getCoordinates(point),
                severity: "EXTREME"
            })
        
        # Heavy precipitation
        IF precipitation[point] > 50:  # mm in 24 hours
            warnings.append(WeatherAlert{
                type: "FLOOD",
                location: getCoordinates(point),
                severity: "HIGH"
            })
    
    RETURN forecast, precipitation, extractSurfaceTemperature(forecast[-1]), warnings
END FUNCTION

â–¶ NUMERICAL METHODS:
  â€¢ Finite Difference: 2nd order accurate in space
  â€¢ Time Integration: Leapfrog scheme with Robert-Asselin filter
  â€¢ CFL Condition: Ensures numerical stability
  â€¢ Grid Staggering: Arakawa C-grid for better wave representation
```

---

## Linguistics

### ğŸ’¬ N-Gram Text Prediction
**Purpose**: Predicts next word based on context (autocomplete)
**Real Usage**: Google Keyboard, SwiftKey, GPT models' foundation

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Context-Aware Text Prediction with Smoothing           â•‘
â•‘ Model: Trigram with Kneser-Ney | Accuracy: ~40% top-3            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  corpus: array[string]        # Training text documents
  context: string              # Current text being typed
  n: int                       # N-gram size (typically 3 for trigram)
  num_predictions: int         # Number of suggestions to return
  
â–¶ OUTPUT:
  predictions: array[Prediction]  # Ranked word suggestions
  confidence: array[float]        # Probability scores

â–¶ DATA STRUCTURES:
  NGramModel: {
    counts: HashMap[tuple[words], HashMap[word, count]]
    vocabulary: Set[string]
    total_words: int
    discount: float  # Kneser-Ney discount parameter
  }
  
  Prediction: {
    word: string
    probability: float
    context_score: float
  }

â–¶ ALGORITHM:
FUNCTION predictNextWord(corpus, context, n, num_predictions):
    
    â•â•â•â•â•â• STEP 1: Build N-Gram Language Model â•â•â•â•â•â•
    model â† NGramModel{
        counts: new HashMap(),
        vocabulary: new Set(),
        total_words: 0,
        discount: 0.75  # Empirically determined
    }
    
    # Tokenize and count n-grams
    FOR document IN corpus:
        tokens â† tokenize(toLowerCase(document))
        tokens â† ["<START>"] + tokens + ["<END>"]
        
        FOR i FROM n-1 TO length(tokens)-1:
            # Extract n-gram context and next word
            ngram_context â† tuple(tokens[i-n+1:i])
            next_word â† tokens[i]
            
            # Update counts
            IF ngram_context NOT IN model.counts:
                model.counts[ngram_context] â† new HashMap()
            
            IF next_word IN model.counts[ngram_context]:
                model.counts[ngram_context][next_word] += 1
            ELSE:
                model.counts[ngram_context][next_word] â† 1
            
            model.vocabulary.add(next_word)
            model.total_words += 1
    
    â•â•â•â•â•â• STEP 2: Apply Kneser-Ney Smoothing â•â•â•â•â•â•
    # Calculate continuation probabilities for unseen n-grams
    continuation_counts â† new HashMap()
    
    FOR context IN model.counts.keys():
        FOR word IN model.counts[context].keys():
            IF word NOT IN continuation_counts:
                continuation_counts[word] â† 0
            continuation_counts[word] += 1
    
    â•â•â•â•â•â• STEP 3: Process Current Context â•â•â•â•â•â•
    context_tokens â† tokenize(toLowerCase(context))
    
    # Get last (n-1) words as context
    IF length(context_tokens) >= n-1:
        query_context â† tuple(context_tokens[-(n-1):])
    ELSE:
        # Pad with start tokens if needed
        padding â† ["<START>"] Ã— (n-1 - length(context_tokens))
        query_context â† tuple(padding + context_tokens)
    
    â•â•â•â•â•â• STEP 4: Calculate Probabilities for All Words â•â•â•â•â•â•
    candidates â† new PriorityQueue()  # Max heap by probability
    
    FOR word IN model.vocabulary:
        IF word == "<START>" OR word == "<END>":
            CONTINUE  # Skip special tokens
        
        # Calculate probability with smoothing
        probability â† calculateSmoothedProbability(
            word, query_context, model, continuation_counts
        )
        
        # Apply context-specific adjustments
        context_score â† calculateContextScore(word, context_tokens)
        
        # Combine scores
        final_score â† probability Ã— 0.7 + context_score Ã— 0.3
        
        candidates.add(Prediction{
            word: word,
            probability: probability,
            context_score: final_score
        })
    
    â•â•â•â•â•â• STEP 5: Select Top Predictions â•â•â•â•â•â•
    predictions â† []
    confidence â† []
    
    FOR i FROM 0 TO num_predictions-1:
        IF candidates.isEmpty():
            BREAK
        
        prediction â† candidates.pop()
        predictions.append(prediction)
        confidence.append(prediction.probability)
    
    # Normalize confidence scores
    total_confidence â† SUM(confidence)
    IF total_confidence > 0:
        FOR i FROM 0 TO length(confidence)-1:
            confidence[i] â† confidence[i] / total_confidence
    
    RETURN predictions, confidence
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION calculateSmoothedProbability(word, context, model, continuation_counts):
    # Kneser-Ney smoothing formula
    IF context IN model.counts AND word IN model.counts[context]:
        count â† model.counts[context][word]
        context_total â† SUM(model.counts[context].values())
        num_word_types â† length(model.counts[context].keys())
        
        # Discounted probability
        discounted â† MAX(count - model.discount, 0) / context_total
        
        # Interpolation weight
        lambda_weight â† (model.discount Ã— num_word_types) / context_total
        
        # Continuation probability (lower-order model)
        IF word IN continuation_counts:
            continuation_prob â† continuation_counts[word] / model.total_words
        ELSE:
            continuation_prob â† 1 / length(model.vocabulary)  # Uniform backoff
        
        RETURN discounted + lambda_weight Ã— continuation_prob
    
    ELSE:
        # Backoff to lower-order model
        IF length(context) > 1:
            shorter_context â† context[1:]  # Remove first word
            RETURN 0.4 Ã— calculateSmoothedProbability(word, shorter_context, model, continuation_counts)
        ELSE:
            # Unigram probability
            IF word IN continuation_counts:
                RETURN continuation_counts[word] / model.total_words
            ELSE:
                RETURN 1 / length(model.vocabulary)
END FUNCTION

FUNCTION calculateContextScore(word, context_tokens):
    score â† 0.0
    
    # Boost common word patterns
    IF lastWord(context_tokens) == "the" AND isNoun(word):
        score += 0.3
    
    IF lastWord(context_tokens) == "to" AND isVerb(word):
        score += 0.3
    
    # Penalize repeating recent words
    IF word IN context_tokens[-5:]:
        score -= 0.2
    
    # Boost based on word frequency in general English
    score += getWordFrequencyScore(word) Ã— 0.1
    
    RETURN SIGMOID(score)  # Normalize to [0,1]
END FUNCTION

â–¶ OPTIMIZATION NOTES:
  â€¢ Use tries for efficient prefix matching
  â€¢ Cache frequent n-gram lookups
  â€¢ Implement lazy loading for large models
  â€¢ Consider character-level models for OOV words
```

---

## Sports Analytics

### âš¾ Sabermetrics Player Valuation
**Purpose**: Calculates player value using advanced statistics
**Real Usage**: MLB teams, FanDuel, DraftKings optimization

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Wins Above Replacement (WAR) Calculation               â•‘
â•‘ League: MLB | Accuracy: r=0.89 with actual wins                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  player_stats: PlayerStats     # Season batting/pitching statistics
  league_stats: LeagueStats     # League averages for context
  park_factors: ParkFactors     # Stadium adjustments
  position: string              # Defensive position
  
â–¶ OUTPUT:
  war: float                    # Wins Above Replacement
  components: WARComponents     # Breakdown of value sources
  dollar_value: float          # Estimated contract value

â–¶ DATA STRUCTURES:
  PlayerStats: {
    # Batting
    plate_appearances: int
    hits: int
    doubles: int
    triples: int
    home_runs: int
    walks: int
    strikeouts: int
    stolen_bases: int
    caught_stealing: int
    
    # Fielding
    putouts: int
    assists: int
    errors: int
    innings_played: float
    
    # Pitching (if applicable)
    innings_pitched: float
    earned_runs: int
    strikeouts_pitched: int
    walks_allowed: int
  }

â–¶ ALGORITHM:
FUNCTION calculateWAR(player_stats, league_stats, park_factors, position):
    
    â•â•â•â•â•â• STEP 1: Calculate Batting Runs â•â•â•â•â•â•
    # Calculate weighted On-Base Average (wOBA)
    wOBA_weights â† {
        walk: 0.69,
        hit_by_pitch: 0.72,
        single: 0.88,
        double: 1.27,
        triple: 1.62,
        home_run: 2.10
    }
    
    singles â† player_stats.hits - player_stats.doubles - player_stats.triples - player_stats.home_runs
    
    wOBA â† (wOBA_weights.walk Ã— player_stats.walks +
            wOBA_weights.single Ã— singles +
            wOBA_weights.double Ã— player_stats.doubles +
            wOBA_weights.triple Ã— player_stats.triples +
            wOBA_weights.home_run Ã— player_stats.home_runs) / player_stats.plate_appearances
    
    # Calculate Weighted Runs Created Plus (wRC+)
    league_wOBA â† league_stats.average_wOBA
    wOBA_scale â† 1.15  # Converts to runs scale
    
    wRC â† ((wOBA - league_wOBA) / wOBA_scale) Ã— player_stats.plate_appearances
    
    # Park adjustment
    park_adjusted_wRC â† wRC Ã— (2 - park_factors.run_factor)
    
    batting_runs â† park_adjusted_wRC
    
    â•â•â•â•â•â• STEP 2: Calculate Baserunning Runs â•â•â•â•â•â•
    # Stolen base runs
    sb_runs â† player_stats.stolen_bases Ã— 0.2
    cs_runs â† player_stats.caught_stealing Ã— -0.4
    
    # Extra base advancement (simplified)
    advancement_opportunities â† singles + player_stats.walks
    expected_advancement â† advancement_opportunities Ã— league_stats.advancement_rate
    actual_advancement â† estimateAdvancement(player_stats)
    
    baserunning_runs â† sb_runs + cs_runs + (actual_advancement - expected_advancement) Ã— 0.3
    
    â•â•â•â•â•â• STEP 3: Calculate Fielding Runs â•â•â•â•â•â•
    IF position != "DH":
        # Ultimate Zone Rating (UZR) calculation
        position_avg_plays â† league_stats.plays_per_position[position]
        
        # Range runs
        plays_made â† player_stats.putouts + player_stats.assists
        expected_plays â† position_avg_plays Ã— (player_stats.innings_played / 1458)  # Full season
        range_runs â† (plays_made - expected_plays) Ã— 0.75
        
        # Error runs
        expected_errors â† league_stats.error_rate[position] Ã— plays_made
        error_runs â† (expected_errors - player_stats.errors) Ã— 0.5
        
        # Double play runs (for middle infielders)
        IF position IN ["2B", "SS"]:
            dp_opportunities â† estimateDoublePlayOpportunities(player_stats)
            expected_dp â† dp_opportunities Ã— league_stats.dp_conversion_rate
            actual_dp â† player_stats.double_plays
            dp_runs â† (actual_dp - expected_dp) Ã— 0.4
        ELSE:
            dp_runs â† 0
        
        fielding_runs â† range_runs + error_runs + dp_runs
    ELSE:
        fielding_runs â† 0
    
    â•â•â•â•â•â• STEP 4: Calculate Positional Adjustment â•â•â•â•â•â•
    positional_adjustment â† {
        "C": +12.5,    # Catcher
        "SS": +7.5,    # Shortstop
        "2B": +2.5,    # Second base
        "3B": +2.5,    # Third base
        "CF": +2.5,    # Center field
        "RF": -7.5,    # Right field
        "LF": -7.5,    # Left field
        "1B": -12.5,   # First base
        "DH": -17.5    # Designated hitter
    }
    
    position_runs â† positional_adjustment[position] Ã— (player_stats.innings_played / 1458)
    
    â•â•â•â•â•â• STEP 5: Calculate Replacement Level â•â•â•â•â•â•
    # Replacement level is ~48% winning percentage
    replacement_level_runs â† 20  # Runs per 600 PA
    replacement_adjustment â† replacement_level_runs Ã— (player_stats.plate_appearances / 600)
    
    â•â•â•â•â•â• STEP 6: Convert Runs to Wins â•â•â•â•â•â•
    runs_per_win â† 10  # Approximately 10 runs = 1 win
    
    total_runs â† batting_runs + baserunning_runs + fielding_runs + position_runs + replacement_adjustment
    
    war â† total_runs / runs_per_win
    
    â•â•â•â•â•â• STEP 7: Calculate Dollar Value â•â•â•â•â•â•
    # Current market: ~$8 million per WAR
    dollars_per_war â† 8000000
    
    # Apply aging curve
    age â† player_stats.age
    IF age < 27:
        age_multiplier â† 1.1  # Peak years ahead
    ELSE IF age < 30:
        age_multiplier â† 1.0  # Peak years
    ELSE IF age < 33:
        age_multiplier â† 0.9  # Slight decline
    ELSE:
        age_multiplier â† 0.7  # Steeper decline
    
    dollar_value â† war Ã— dollars_per_war Ã— age_multiplier
    
    â•â•â•â•â•â• STEP 8: Compile Components â•â•â•â•â•â•
    components â† WARComponents{
        batting: batting_runs / runs_per_win,
        baserunning: baserunning_runs / runs_per_win,
        fielding: fielding_runs / runs_per_win,
        positional: position_runs / runs_per_win,
        replacement: replacement_adjustment / runs_per_win,
        total: war
    }
    
    RETURN war, components, dollar_value
END FUNCTION

â–¶ VALIDATION:
  â€¢ Correlate with team wins (r > 0.85)
  â€¢ Compare across calculation methods (fWAR, bWAR, WARP)
  â€¢ Adjust for sample size (minimum 100 PA)
  â€¢ Account for league and era differences
```

---

## Everyday Applications

### ğŸš— Ride-Sharing Driver-Passenger Matching
**Purpose**: Optimally matches drivers with passengers
**Real Usage**: Uber, Lyft, DiDi real-time matching systems

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Bipartite Matching with Dynamic Pricing                â•‘
â•‘ Optimization: Hungarian Algorithm | Time: O(nÂ³)                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  drivers: array[Driver]         # Available drivers
  passengers: array[Passenger]   # Waiting passengers
  max_wait_time: int            # Maximum acceptable wait (seconds)
  surge_threshold: float        # Demand/supply ratio for surge
  
â–¶ OUTPUT:
  matches: array[Match]         # Driver-passenger pairs
  unmatched: array[Passenger]   # Passengers still waiting
  surge_zones: array[SurgeZone] # Areas with surge pricing

â–¶ DATA STRUCTURES:
  Driver: {
    id: string
    location: Coordinates
    rating: float
    vehicle_type: string
    current_trip_end: datetime
  }
  
  Passenger: {
    id: string
    pickup: Coordinates
    destination: Coordinates
    request_time: datetime
    max_price: float
    required_vehicle: string
  }
  
  Match: {
    driver_id: string
    passenger_id: string
    pickup_time: datetime
    fare: float
    distance: float
  }

â–¶ ALGORITHM:
FUNCTION matchRiders(drivers, passengers, max_wait_time, surge_threshold):
    
    â•â•â•â•â•â• STEP 1: Calculate Supply-Demand Ratio â•â•â•â•â•â•
    # Divide city into hexagonal grid cells (H3 indexing)
    grid_cells â† createHexGrid(city_bounds, resolution=8)  # ~460m cells
    
    demand_map â† new HashMap()
    supply_map â† new HashMap()
    
    FOR passenger IN passengers:
        cell â† getHexCell(passenger.pickup)
        demand_map[cell] â† demand_map[cell] + 1
    
    FOR driver IN drivers:
        cell â† getHexCell(driver.location)
        IF driver.current_trip_end < NOW():
            supply_map[cell] â† supply_map[cell] + 1
    
    â•â•â•â•â•â• STEP 2: Calculate Surge Pricing â•â•â•â•â•â•
    surge_zones â† []
    
    FOR cell IN grid_cells:
        demand â† demand_map[cell] OR 0
        supply â† supply_map[cell] OR 1  # Avoid division by zero
        
        ratio â† demand / supply
        
        IF ratio > surge_threshold:
            # Surge multiplier: 1.2x to 3.0x cap
            surge_multiplier â† MIN(1 + (ratio - surge_threshold) Ã— 0.5, 3.0)
            
            surge_zones.append(SurgeZone{
                cell: cell,
                multiplier: surge_multiplier,
                demand: demand,
                supply: supply
            })
    
    â•â•â•â•â•â• STEP 3: Build Cost Matrix â•â•â•â•â•â•
    n_drivers â† length(drivers)
    n_passengers â† length(passengers)
    
    # Initialize cost matrix with "infinity" for impossible matches
    cost_matrix â† array[n_drivers][n_passengers]
    FILL(cost_matrix, INFINITY)
    
    FOR i FROM 0 TO n_drivers-1:
        FOR j FROM 0 TO n_passengers-1:
            driver â† drivers[i]
            passenger â† passengers[j]
            
            # Check vehicle type compatibility
            IF NOT isCompatible(driver.vehicle_type, passenger.required_vehicle):
                CONTINUE  # Keep as INFINITY
            
            # Calculate pickup distance and time
            pickup_distance â† haversineDistance(driver.location, passenger.pickup)
            pickup_time â† pickup_distance / AVERAGE_SPEED  # ~25 km/h in city
            
            # Check time constraints
            wait_time â† NOW() - passenger.request_time + pickup_time
            IF wait_time > max_wait_time:
                CONTINUE  # Keep as INFINITY
            
            # Calculate trip distance
            trip_distance â† routeDistance(passenger.pickup, passenger.destination)
            
            # Calculate base fare
            base_fare â† 2.50 + (1.50 Ã— trip_distance) + (0.25 Ã— pickup_time/60)
            
            # Apply surge pricing if applicable
            cell â† getHexCell(passenger.pickup)
            surge â† getSurgeMultiplier(surge_zones, cell)
            fare â† base_fare Ã— surge
            
            # Check passenger's maximum price
            IF fare > passenger.max_price:
                CONTINUE  # Keep as INFINITY
            
            # Cost function combines multiple factors
            driver_score â† (5 - driver.rating) Ã— 2  # Prefer higher-rated drivers
            distance_cost â† pickup_distance Ã— 0.5   # Minimize pickup distance
            time_cost â† wait_time Ã— 0.01            # Minimize wait time
            
            cost_matrix[i][j] â† distance_cost + time_cost + driver_score
    
    â•â•â•â•â•â• STEP 4: Hungarian Algorithm for Optimal Matching â•â•â•â•â•â•
    # Find minimum cost perfect matching
    matches â† []
    
    # Step 4.1: Subtract row minimums
    FOR i FROM 0 TO n_drivers-1:
        row_min â† MIN(cost_matrix[i])
        IF row_min < INFINITY:
            FOR j FROM 0 TO n_passengers-1:
                cost_matrix[i][j] â† cost_matrix[i][j] - row_min
    
    # Step 4.2: Subtract column minimums
    FOR j FROM 0 TO n_passengers-1:
        col_min â† MIN(cost_matrix[:][j])
        IF col_min < INFINITY:
            FOR i FROM 0 TO n_drivers-1:
                cost_matrix[i][j] â† cost_matrix[i][j] - col_min
    
    # Step 4.3: Find augmenting paths (simplified)
    assignment â† array[n_drivers]
    FILL(assignment, -1)
    
    WHILE hasUnmatchedDrivers(assignment):
        # Find augmenting path using BFS
        path â† findAugmentingPath(cost_matrix, assignment)
        
        IF path == NULL:
            # Adjust cost matrix
            adjustCostMatrix(cost_matrix)
        ELSE:
            # Apply augmenting path
            applyPath(assignment, path)
    
    â•â•â•â•â•â• STEP 5: Create Final Matches â•â•â•â•â•â•
    FOR i FROM 0 TO n_drivers-1:
        IF assignment[i] != -1:
            j â† assignment[i]
            
            driver â† drivers[i]
            passenger â† passengers[j]
            
            pickup_distance â† haversineDistance(driver.location, passenger.pickup)
            pickup_time â† NOW() + (pickup_distance / AVERAGE_SPEED) Ã— 3600
            
            trip_distance â† routeDistance(passenger.pickup, passenger.destination)
            
            # Recalculate fare with final surge
            base_fare â† 2.50 + (1.50 Ã— trip_distance) + (0.25 Ã— pickup_distance)
            cell â† getHexCell(passenger.pickup)
            surge â† getSurgeMultiplier(surge_zones, cell)
            final_fare â† base_fare Ã— surge
            
            matches.append(Match{
                driver_id: driver.id,
                passenger_id: passenger.id,
                pickup_time: pickup_time,
                fare: final_fare,
                distance: trip_distance
            })
    
    â•â•â•â•â•â• STEP 6: Identify Unmatched Passengers â•â•â•â•â•â•
    matched_passengers â† SET(match.passenger_id FOR match IN matches)
    unmatched â† []
    
    FOR passenger IN passengers:
        IF passenger.id NOT IN matched_passengers:
            unmatched.append(passenger)
    
    RETURN matches, unmatched, surge_zones
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION haversineDistance(coord1, coord2):
    R â† 6371  # Earth radius in km
    
    lat1, lon1 â† coord1.lat, coord1.lon
    lat2, lon2 â† coord2.lat, coord2.lon
    
    dlat â† toRadians(lat2 - lat1)
    dlon â† toRadians(lon2 - lon1)
    
    a â† sin(dlat/2)Â² + cos(toRadians(lat1)) Ã— cos(toRadians(lat2)) Ã— sin(dlon/2)Â²
    c â† 2 Ã— atan2(sqrt(a), sqrt(1-a))
    
    RETURN R Ã— c
END FUNCTION

â–¶ REAL-TIME OPTIMIZATIONS:
  â€¢ Batch matching every 5-10 seconds
  â€¢ Use spatial indexing (R-tree) for nearby drivers
  â€¢ Cache route distances between common locations
  â€¢ Implement driver repositioning suggestions
  â€¢ Use ML to predict demand surges
```

---

## Recommendation Systems

### ğŸ“º Collaborative Filtering (Netflix/Spotify Style)
**Purpose**: Recommends content based on user behavior patterns
**Real Usage**: Netflix (75% of views), Spotify Discover Weekly

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Matrix Factorization with Implicit Feedback            â•‘
â•‘ Method: Alternating Least Squares | Scale: 100M+ users            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  user_interactions: sparse_matrix[users Ã— items]  # View counts/play time
  num_factors: int                                 # Latent factors (typically 50-200)
  regularization: float                            # Prevent overfitting (Î»)
  iterations: int                                  # Training iterations
  
â–¶ OUTPUT:
  recommendations: map[user_id, array[item_id]]    # Top-N recommendations
  user_embeddings: matrix[users Ã— factors]         # Learned user preferences
  item_embeddings: matrix[items Ã— factors]         # Learned item features

â–¶ ALGORITHM:
FUNCTION collaborativeFilter(user_interactions, num_factors, regularization, iterations):
    
    â•â•â•â•â•â• STEP 1: Prepare Interaction Matrix â•â•â•â•â•â•
    n_users â† rows(user_interactions)
    n_items â† columns(user_interactions)
    
    # Convert implicit feedback to confidence scores
    confidence_matrix â† sparse_matrix[n_users][n_items]
    preference_matrix â† sparse_matrix[n_users][n_items]
    
    Î± â† 40  # Confidence scaling parameter
    
    FOR user FROM 0 TO n_users-1:
        FOR item IN user_interactions[user].nonzero():
            interaction_count â† user_interactions[user][item]
            
            # Binary preference: 1 if interacted, 0 otherwise
            preference_matrix[user][item] â† 1 if interaction_count > 0 else 0
            
            # Confidence increases with more interactions
            confidence_matrix[user][item] â† 1 + Î± Ã— log(1 + interaction_count)
    
    â•â•â•â•â•â• STEP 2: Initialize Factor Matrices â•â•â•â•â•â•
    # Random initialization with small values
    user_factors â† randomMatrix(n_users, num_factors, mean=0, std=0.01)
    item_factors â† randomMatrix(n_items, num_factors, mean=0, std=0.01)
    
    â•â•â•â•â•â• STEP 3: Alternating Least Squares Optimization â•â•â•â•â•â•
    FOR iteration FROM 0 TO iterations-1:
        
        â”€â”€â”€ SUBSTEP 3.1: Fix Items, Solve for Users â”€â”€â”€
        FOR user FROM 0 TO n_users-1:
            # Build weighted regularization matrix
            Cu â† diagonalMatrix(confidence_matrix[user])
            
            # Compute: (Y^T Ã— Cu Ã— Y + Î»I)^(-1) Ã— Y^T Ã— Cu Ã— p(u)
            YtCuY â† item_factors.T Ã— Cu Ã— item_factors
            YtCuY â† YtCuY + regularization Ã— identity(num_factors)
            
            YtCupu â† item_factors.T Ã— Cu Ã— preference_matrix[user]
            
            # Solve linear system
            user_factors[user] â† solve(YtCuY, YtCupu)
        
        â”€â”€â”€ SUBSTEP 3.2: Fix Users, Solve for Items â”€â”€â”€
        FOR item FROM 0 TO n_items-1:
            # Build weighted regularization matrix
            Ci â† diagonalMatrix(confidence_matrix[:, item])
            
            # Compute: (X^T Ã— Ci Ã— X + Î»I)^(-1) Ã— X^T Ã— Ci Ã— p(i)
            XtCiX â† user_factors.T Ã— Ci Ã— user_factors
            XtCiX â† XtCiX + regularization Ã— identity(num_factors)
            
            XtCipi â† user_factors.T Ã— Ci Ã— preference_matrix[:, item]
            
            # Solve linear system
            item_factors[item] â† solve(XtCiX, XtCipi)
        
        â”€â”€â”€ SUBSTEP 3.3: Calculate Loss (Optional) â”€â”€â”€
        IF iteration % 10 == 0:
            loss â† 0
            FOR user FROM 0 TO n_users-1:
                FOR item IN confidence_matrix[user].nonzero():
                    prediction â† dot(user_factors[user], item_factors[item])
                    error â† preference_matrix[user][item] - prediction
                    loss += confidence_matrix[user][item] Ã— errorÂ²
            
            loss += regularization Ã— (norm(user_factors)Â² + norm(item_factors)Â²)
            PRINT f"Iteration {iteration}: Loss = {loss}"
    
    â•â•â•â•â•â• STEP 4: Generate Recommendations â•â•â•â•â•â•
    recommendations â† new HashMap()
    
    FOR user FROM 0 TO n_users-1:
        # Calculate scores for all items
        scores â† user_factors[user] Ã— item_factors.T
        
        # Get items user has already interacted with
        seen_items â† SET(user_interactions[user].nonzero())
        
        # Create candidate pool
        candidates â† []
        FOR item FROM 0 TO n_items-1:
            IF item NOT IN seen_items:
                candidates.append((item, scores[item]))
        
        # Sort by score and take top-N
        candidates â† SORT(candidates, key=score, descending=True)
        recommendations[user] â† [item_id FOR (item_id, score) IN candidates[:100]]
        
        # Apply business rules and filters
        recommendations[user] â† applyFilters(recommendations[user], user)
    
    â•â•â•â•â•â• STEP 5: Apply Diversity and Freshness â•â•â•â•â•â•
    FOR user IN recommendations.keys():
        recs â† recommendations[user]
        
        # Ensure genre diversity
        diverse_recs â† []
        genre_counts â† new HashMap()
        max_per_genre â† 3
        
        FOR item IN recs:
            genre â† getItemGenre(item)
            IF genre_counts[genre] < max_per_genre:
                diverse_recs.append(item)
                genre_counts[genre] += 1
            
            IF length(diverse_recs) >= 20:
                BREAK
        
        # Add fresh/trending content
        trending_items â† getTrendingItems(limit=5)
        diverse_recs â† trending_items + diverse_recs[:15]
        
        recommendations[user] â† diverse_recs
    
    RETURN recommendations, user_factors, item_factors
END FUNCTION

â–¶ BUSINESS LOGIC FILTERS:
FUNCTION applyFilters(recommendations, user):
    filtered â† []
    user_profile â† getUserProfile(user)
    
    FOR item IN recommendations:
        # Content appropriateness
        IF NOT isAgeAppropriate(item, user_profile.age):
            CONTINUE
        
        # Regional availability
        IF NOT isAvailableInRegion(item, user_profile.region):
            CONTINUE
        
        # Recency bias (boost newer content)
        IF getItemAge(item) < 30:  # days
            filtered.insert(0, item)  # Add to front
        ELSE:
            filtered.append(item)
    
    RETURN filtered[:20]  # Return top 20
END FUNCTION

â–¶ SCALABILITY OPTIMIZATIONS:
  â€¢ Use sparse matrix operations (scipy.sparse)
  â€¢ Parallelize user/item updates across cores
  â€¢ Implement incremental learning for new users
  â€¢ Use approximate nearest neighbors (Annoy, FAISS)
  â€¢ Cache frequently accessed embeddings
```

---

## Summary

These algorithms demonstrate how computational thinking extends far beyond traditional tech domains:

1. **Biology**: DNA alignment enables personalized medicine
2. **Finance**: Trading algorithms move billions daily
3. **Arts**: AI composers create original music
4. **Climate**: Weather models save lives through predictions
5. **Language**: Text prediction speeds communication
6. **Sports**: Analytics revolutionized team strategies
7. **Transportation**: Matching algorithms optimize city movement
8. **Entertainment**: Recommendations drive 75%+ of consumption

Each algorithm shown here is actively deployed in production systems, affecting millions of people daily. The key insight is that algorithms are simply structured problem-solving approachesâ€”applicable wherever patterns exist and decisions need optimization.