# Real-World Sort Algorithms: Comprehensive Examples with Pseudocode

## Table of Contents
1. [Database External Merge Sort](#database-external-merge-sort)
2. [Video Streaming Adaptive Bitrate](#video-streaming-adaptive-bitrate)
3. [Search Engine Result Ranking](#search-engine-result-ranking)
4. [Financial Trading Order Book](#financial-trading-order-book)
5. [Genomic Sequence Analysis](#genomic-sequence-analysis)
6. [Social Media Feed Ranking](#social-media-feed-ranking)
7. [E-Commerce Product Sorting](#e-commerce-product-sorting)
8. [Distributed TeraSort](#distributed-terasort)

---

## Database External Merge Sort

### ğŸ’¾ Large-Scale Database Index Creation
**Purpose**: Sorts terabytes of data that don't fit in memory
**Real Usage**: PostgreSQL CLUSTER, MySQL ALTER TABLE ORDER BY, Oracle index rebuilds

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: K-Way External Merge Sort with Replacement Selection   â•‘
â•‘ Scale: Terabytes | Memory: 1GB | Disk I/O Optimized               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  input_file: File                   # Unsorted data file (TB size)
  output_file: File                  # Sorted output destination
  memory_limit: int                  # Available RAM in bytes
  key_columns: array[Column]         # Columns to sort by
  sort_order: array[Direction]       # ASC/DESC per column
  
â–¶ OUTPUT:
  sorted_file: File                  # Fully sorted data
  statistics: SortStats              # Performance metrics
  index_metadata: IndexInfo          # For database index creation

â–¶ DATA STRUCTURES:
  Record: {
    key: array[Value]               # Sort key values
    data: ByteArray                 # Full record data
    file_offset: long               # Original position
  }
  
  Run: {
    file_path: string               # Temporary file path
    num_records: long               # Records in this run
    key_range: (min_key, max_key)   # For optimization
    size_bytes: long
  }
  
  HeapNode: {
    record: Record
    run_index: int                  # Which run this came from
  }

â–¶ ALGORITHM PHASE 1: Generate Initial Runs
FUNCTION generateSortedRuns(input_file, memory_limit, key_columns):
    
    â•â•â•â•â•â• STEP 1: Calculate Buffer Sizes â•â•â•â•â•â•
    # Reserve memory for merge phase
    sort_buffer_size â† memory_limit Ã— 0.8
    io_buffer_size â† memory_limit Ã— 0.2
    
    records_per_buffer â† sort_buffer_size / AVERAGE_RECORD_SIZE
    
    runs â† []
    run_id â† 0
    
    â•â•â•â•â•â• STEP 2: Replacement Selection for Longer Runs â•â•â•â•â•â•
    # Use heap for generating runs 2x memory size on average
    heap â† MinHeap(key=lambda r: extractKey(r, key_columns))
    current_run_key â† NULL
    current_run â† []
    
    input_buffer â† BufferedReader(input_file, io_buffer_size)
    
    # Fill initial heap
    WHILE heap.size() < records_per_buffer AND NOT input_buffer.eof():
        record â† parseRecord(input_buffer.readLine())
        heap.push(record)
    
    WHILE NOT heap.isEmpty():
        # Get minimum record
        min_record â† heap.pop()
        
        # Check if record belongs to current run
        IF current_run_key == NULL OR 
           compareKeys(min_record.key, current_run_key) >= 0:
            # Add to current run
            current_run.append(min_record)
            current_run_key â† min_record.key
        ELSE:
            # Start new run
            IF length(current_run) > 0:
                run â† writeRun(current_run, run_id)
                runs.append(run)
                run_id += 1
            
            current_run â† [min_record]
            current_run_key â† min_record.key
        
        # Read next record if available
        IF NOT input_buffer.eof():
            record â† parseRecord(input_buffer.readLine())
            heap.push(record)
        
        # Periodic memory check
        IF heap.size() > records_per_buffer:
            # Flush smaller records to disk
            overflow â† []
            WHILE heap.size() > records_per_buffer Ã— 0.9:
                overflow.append(heap.pop())
            
            # Write overflow as separate run
            overflow_run â† writeRun(overflow, run_id)
            runs.append(overflow_run)
            run_id += 1
    
    # Write final run
    IF length(current_run) > 0:
        run â† writeRun(current_run, run_id)
        runs.append(run)
    
    RETURN runs
    
    â•â•â•â•â•â• HELPER: Write Run to Disk â•â•â•â•â•â•
    FUNCTION writeRun(records, run_id):
        # In-memory sort before writing
        records â† quicksortWithMultipleKeys(records, key_columns, sort_order)
        
        run_file â† f"temp_run_{run_id}.dat"
        writer â† BufferedWriter(run_file, io_buffer_size)
        
        min_key â† records[0].key
        max_key â† records[-1].key
        bytes_written â† 0
        
        FOR record IN records:
            serialized â† serializeRecord(record)
            writer.write(serialized)
            bytes_written += length(serialized)
        
        writer.flush()
        writer.close()
        
        RETURN Run{
            file_path: run_file,
            num_records: length(records),
            key_range: (min_key, max_key),
            size_bytes: bytes_written
        }
    END FUNCTION
END FUNCTION

â–¶ ALGORITHM PHASE 2: K-Way Merge
FUNCTION kWayMerge(runs, output_file, memory_limit, key_columns):
    
    â•â•â•â•â•â• STEP 1: Determine Merge Fan-in â•â•â•â•â•â•
    k â† length(runs)
    
    # Calculate optimal buffer size per run
    buffer_per_run â† memory_limit / (k + 1)  # +1 for output buffer
    
    IF buffer_per_run < MIN_BUFFER_SIZE:
        # Need multiple merge passes
        RETURN multiPassMerge(runs, output_file, memory_limit, key_columns)
    
    â•â•â•â•â•â• STEP 2: Initialize Run Readers â•â•â•â•â•â•
    readers â† []
    merge_heap â† MinHeap(key=lambda node: node.record.key)
    
    FOR i, run IN enumerate(runs):
        reader â† BufferedReader(run.file_path, buffer_per_run)
        readers.append(reader)
        
        # Read first record from each run
        IF NOT reader.eof():
            record â† parseRecord(reader.readLine())
            merge_heap.push(HeapNode{
                record: record,
                run_index: i
            })
    
    â•â•â•â•â•â• STEP 3: Merge All Runs â•â•â•â•â•â•
    output_buffer â† BufferedWriter(output_file, buffer_per_run)
    records_written â† 0
    last_key â† NULL
    
    WHILE NOT merge_heap.isEmpty():
        # Get minimum record across all runs
        min_node â† merge_heap.pop()
        
        # Check sort order integrity
        IF last_key != NULL:
            IF compareKeys(min_node.record.key, last_key) < 0:
                ERROR("Sort order violation detected!")
        
        # Write to output
        output_buffer.write(serializeRecord(min_node.record))
        records_written += 1
        last_key â† min_node.record.key
        
        # Read next record from the same run
        run_index â† min_node.run_index
        IF NOT readers[run_index].eof():
            next_record â† parseRecord(readers[run_index].readLine())
            merge_heap.push(HeapNode{
                record: next_record,
                run_index: run_index
            })
        
        # Periodic buffer flush
        IF records_written % 10000 == 0:
            output_buffer.flush()
    
    # Final flush and cleanup
    output_buffer.flush()
    output_buffer.close()
    
    FOR reader IN readers:
        reader.close()
    
    # Delete temporary run files
    FOR run IN runs:
        deleteFile(run.file_path)
    
    RETURN records_written
END FUNCTION

â–¶ ALGORITHM PHASE 3: Multi-Pass Merge for Large K
FUNCTION multiPassMerge(runs, output_file, memory_limit, key_columns):
    
    â•â•â•â•â•â• Calculate Merge Tree â•â•â•â•â•â•
    # Determine fan-in per pass
    max_fan_in â† memory_limit / MIN_BUFFER_SIZE - 1
    
    current_runs â† runs
    pass_number â† 0
    
    WHILE length(current_runs) > 1:
        next_runs â† []
        
        # Process runs in groups
        FOR i FROM 0 TO length(current_runs) STEP max_fan_in:
            group â† current_runs[i:MIN(i + max_fan_in, length(current_runs))]
            
            IF length(group) == 1:
                # Single run, just promote to next level
                next_runs.append(group[0])
            ELSE:
                # Merge this group
                merged_run_file â† f"pass_{pass_number}_run_{i/max_fan_in}.dat"
                merged_records â† kWayMerge(
                    group,
                    merged_run_file,
                    memory_limit,
                    key_columns
                )
                
                next_runs.append(Run{
                    file_path: merged_run_file,
                    num_records: SUM(r.num_records FOR r IN group),
                    key_range: (
                        MIN(r.key_range[0] FOR r IN group),
                        MAX(r.key_range[1] FOR r IN group)
                    )
                })
        
        current_runs â† next_runs
        pass_number += 1
    
    # Final run is our sorted output
    IF current_runs[0].file_path != output_file:
        moveFile(current_runs[0].file_path, output_file)
    
    RETURN current_runs[0].num_records
END FUNCTION

â–¶ OPTIMIZATIONS FOR DATABASE INDEXES:
FUNCTION createDatabaseIndex(sorted_file, table_name, index_name):
    
    â•â•â•â•â•â• Build B+ Tree Index â•â•â•â•â•â•
    page_size â† 8192  # 8KB pages
    index_file â† f"{index_name}.idx"
    
    # Calculate node capacity
    key_size â† estimateKeySize(key_columns)
    pointer_size â† 8  # bytes
    keys_per_node â† (page_size - pointer_size) / (key_size + pointer_size)
    
    # Build index bottom-up
    leaf_pages â† []
    current_page â† createEmptyPage()
    
    reader â† BufferedReader(sorted_file)
    record_offset â† 0
    
    WHILE NOT reader.eof():
        record â† parseRecord(reader.readLine())
        
        # Add to current leaf page
        IF current_page.num_keys < keys_per_node:
            current_page.addKey(record.key, record_offset)
        ELSE:
            # Page full, write and start new one
            leaf_pages.append(writePage(current_page, index_file))
            current_page â† createEmptyPage()
            current_page.addKey(record.key, record_offset)
        
        record_offset += recordSize(record)
    
    # Write final leaf page
    IF current_page.num_keys > 0:
        leaf_pages.append(writePage(current_page, index_file))
    
    # Build internal nodes
    buildInternalNodes(leaf_pages, index_file)
    
    RETURN IndexInfo{
        index_name: index_name,
        index_file: index_file,
        num_leaf_pages: length(leaf_pages),
        height: calculateTreeHeight(length(leaf_pages)),
        key_columns: key_columns
    }
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION quicksortWithMultipleKeys(records, key_columns, sort_order):
    IF length(records) <= 1:
        RETURN records
    
    # Three-way partitioning for equal keys
    pivot â† records[length(records) / 2]
    less â† []
    equal â† []
    greater â† []
    
    FOR record IN records:
        comparison â† compareMultipleKeys(
            record.key,
            pivot.key,
            key_columns,
            sort_order
        )
        
        IF comparison < 0:
            less.append(record)
        ELSE IF comparison == 0:
            equal.append(record)
        ELSE:
            greater.append(record)
    
    RETURN (quicksortWithMultipleKeys(less, key_columns, sort_order) +
            equal +
            quicksortWithMultipleKeys(greater, key_columns, sort_order))
END FUNCTION

FUNCTION compareMultipleKeys(key1, key2, columns, sort_order):
    FOR i, column IN enumerate(columns):
        val1 â† key1[i]
        val2 â† key2[i]
        
        # Handle NULL values (NULLS FIRST or NULLS LAST)
        IF val1 == NULL AND val2 == NULL:
            CONTINUE
        IF val1 == NULL:
            RETURN -1 IF sort_order[i] == ASC ELSE 1
        IF val2 == NULL:
            RETURN 1 IF sort_order[i] == ASC ELSE -1
        
        # Compare based on data type
        IF column.type == "STRING":
            result â† strcoll(val1, val2)  # Locale-aware comparison
        ELSE IF column.type == "NUMERIC":
            result â† val1 - val2
        ELSE IF column.type == "DATE":
            result â† val1.timestamp - val2.timestamp
        
        IF result != 0:
            RETURN result IF sort_order[i] == ASC ELSE -result
    
    RETURN 0  # All keys equal
END FUNCTION

â–¶ PERFORMANCE METRICS:
  â€¢ I/O Operations: O(n Ã— log(n/M)) where M = memory size
  â€¢ CPU Time: O(n Ã— log n)
  â€¢ Temporary Space: O(n)
  â€¢ Typical Performance: 100GB sorted in ~10 minutes on SSD
```

---

## Video Streaming Adaptive Bitrate

### ğŸ“¹ Netflix/YouTube Quality Adaptation
**Purpose**: Sorts video segments by quality for adaptive streaming
**Real Usage**: Netflix, YouTube, Twitch, Disney+ streaming

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Heap-Based Adaptive Bitrate Streaming with Buffer Sort â•‘
â•‘ Latency: <100ms | Bitrates: 10+ levels | Users: Millions          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  video_segments: array[Segment]     # Available quality versions
  network_bandwidth: float           # Current bandwidth (Mbps)
  buffer_health: BufferStatus        # Playback buffer state
  viewport_size: Resolution          # Device screen resolution
  user_preferences: Preferences      # Quality settings
  
â–¶ OUTPUT:
  download_queue: PriorityQueue      # Ordered segments to fetch
  quality_ladder: array[Quality]     # Sorted quality options
  switch_decision: QualitySwitch     # When/how to change quality

â–¶ DATA STRUCTURES:
  Segment: {
    index: int                      # Temporal position
    quality_level: int              # 0=lowest, N=highest
    bitrate: int                    # Bits per second
    resolution: Resolution          # Width Ã— Height
    codec: string                   # H.264, H.265, AV1
    size_bytes: int
    duration_ms: int
    vmaf_score: float              # Perceptual quality (0-100)
    url: string
  }
  
  BufferStatus: {
    current_level_ms: int          # Buffered content duration
    target_level_ms: int           # Desired buffer
    stall_risk: float              # Probability of rebuffering
    history: array[BufferSample]
  }

â–¶ ALGORITHM:
FUNCTION adaptiveBitrateSort(video_segments, bandwidth, buffer, viewport, preferences):
    
    â•â•â•â•â•â• STEP 1: Build Quality Ladder (Heap Sort) â•â•â•â•â•â•
    quality_ladder â† buildQualityLadder(video_segments, viewport)
    
    FUNCTION buildQualityLadder(segments, viewport):
        # Group segments by quality level
        quality_groups â† groupBy(segments, s => s.quality_level)
        
        # Create quality ladder using heap sort
        quality_heap â† MaxHeap(key=lambda q: calculateQualityScore(q, viewport))
        
        FOR level, segments IN quality_groups:
            representative â† segments[0]  # All same quality
            
            # Calculate effective quality score
            score â† calculateQualityScore(representative, viewport)
            
            quality_heap.push(QualityLevel{
                level: level,
                bitrate: representative.bitrate,
                resolution: representative.resolution,
                vmaf: representative.vmaf_score,
                score: score
            })
        
        # Extract sorted ladder (best to worst)
        ladder â† []
        WHILE NOT quality_heap.isEmpty():
            ladder.append(quality_heap.pop())
        
        RETURN ladder
    END FUNCTION
    
    â•â•â•â•â•â• STEP 2: Rate-Based Sorting with Buffer Awareness â•â•â•â•â•â•
    FUNCTION selectSegmentQualities(segments, bandwidth, buffer):
        # Segment download priority queue
        download_queue â† MinHeap(key=lambda s: s.download_priority)
        
        # Current playback position
        current_index â† getCurrentPlaybackIndex()
        
        # Buffer health categories
        buffer_state â† categorizeBuffer(buffer)
        
        FOR segment IN segments[current_index:]:
            # Adaptive quality selection
            selected_quality â† NULL
            
            IF buffer_state == "CRITICAL":
                # Emergency mode - lowest quality only
                selected_quality â† selectLowestQuality(segment, quality_ladder)
                
            ELSE IF buffer_state == "LOW":
                # Conservative - pick sustainable quality
                sustainable_bitrate â† bandwidth Ã— 0.7  # Safety margin
                selected_quality â† selectByBitrate(
                    segment,
                    sustainable_bitrate,
                    quality_ladder
                )
                
            ELSE IF buffer_state == "HEALTHY":
                # Optimal - balance quality and stability
                selected_quality â† selectOptimalQuality(
                    segment,
                    bandwidth,
                    buffer,
                    quality_ladder
                )
                
            ELSE IF buffer_state == "EXCESS":
                # Aggressive - maximize quality
                selected_quality â† selectHighestFeasible(
                    segment,
                    bandwidth Ã— 1.2,  # Optimistic
                    quality_ladder
                )
            
            # Calculate download priority
            priority â† calculatePriority(
                segment,
                selected_quality,
                buffer,
                current_index
            )
            
            download_queue.push(SegmentDownload{
                segment: segment,
                quality: selected_quality,
                priority: priority
            })
        
        RETURN download_queue
    END FUNCTION
    
    â•â•â•â•â•â• STEP 3: Dynamic Quality Switching Logic â•â•â•â•â•â•
    FUNCTION determineQualitySwitch(current_quality, buffer, bandwidth):
        # Smooth quality transitions using hysteresis
        
        # Calculate throughput statistics
        throughput_samples â† getRecentThroughput(10)  # Last 10 segments
        avg_throughput â† MEAN(throughput_samples)
        throughput_variance â† VARIANCE(throughput_samples)
        
        # Estimate sustainable quality
        confidence â† 1 - (throughput_variance / avg_throughputÂ²)
        safe_bandwidth â† avg_throughput Ã— (0.5 + 0.5 Ã— confidence)
        
        # Find best sustainable quality
        target_quality â† NULL
        FOR quality IN quality_ladder:
            IF quality.bitrate <= safe_bandwidth:
                target_quality â† quality
                BREAK  # Ladder is sorted high to low
        
        # Decide on switch timing
        switch_decision â† QualitySwitch{
            from_quality: current_quality,
            to_quality: target_quality,
            when: "IMMEDIATE",
            reason: ""
        }
        
        # Apply switching constraints
        IF ABS(target_quality.level - current_quality.level) > 2:
            # Avoid jarring quality jumps
            intermediate â† findIntermediateQuality(
                current_quality,
                target_quality,
                quality_ladder
            )
            switch_decision.to_quality â† intermediate
            switch_decision.reason â† "GRADUAL_TRANSITION"
            
        ELSE IF buffer.current_level_ms < 2000:
            # Don't increase quality when buffer is low
            IF target_quality.bitrate > current_quality.bitrate:
                switch_decision.to_quality â† current_quality
                switch_decision.when â† "POSTPONE"
                switch_decision.reason â† "BUFFER_TOO_LOW"
                
        ELSE IF throughput_variance > avg_throughput Ã— 0.5:
            # High variance - be conservative
            switch_decision.when â† "AFTER_STABILIZATION"
            switch_decision.reason â† "NETWORK_UNSTABLE"
        
        RETURN switch_decision
    END FUNCTION
    
    â•â•â•â•â•â• STEP 4: Lookahead Sorting for Smooth Playback â•â•â•â•â•â•
    FUNCTION sortWithLookahead(segments, window_size=30):
        # Sort upcoming segments considering future bandwidth
        
        lookahead_window â† segments[0:window_size]
        sorted_segments â† []
        
        # Bandwidth prediction model
        bandwidth_forecast â† predictBandwidth(window_size)
        
        FOR i, segment IN enumerate(lookahead_window):
            predicted_bandwidth â† bandwidth_forecast[i]
            
            # Score each quality option
            quality_scores â† []
            
            FOR quality IN getQualitiesForSegment(segment):
                # Multi-objective scoring
                quality_score â† quality.vmaf_score / 100
                
                # Delivery score (can we download in time?)
                download_time â† quality.size_bytes / predicted_bandwidth
                delivery_score â† 1.0 IF download_time < segment.duration_ms ELSE 0.5
                
                # Smoothness score (avoid quality oscillation)
                IF i > 0:
                    prev_quality â† sorted_segments[i-1].quality
                    smoothness â† 1 - ABS(quality.level - prev_quality.level) / 10
                ELSE:
                    smoothness â† 1.0
                
                # Combined score
                total_score â† (
                    quality_score Ã— 0.4 +
                    delivery_score Ã— 0.4 +
                    smoothness Ã— 0.2
                )
                
                quality_scores.append((quality, total_score))
            
            # Sort by score and select best
            quality_scores â† SORT(quality_scores, key=score, descending=True)
            best_quality â† quality_scores[0][0]
            
            sorted_segments.append(SegmentChoice{
                segment: segment,
                quality: best_quality,
                download_order: i
            })
        
        RETURN sorted_segments
    END FUNCTION
END FUNCTION

â–¶ SPECIALIZED SORTING: Live Stream Buffer Management
FUNCTION manageLiveStreamBuffer(incoming_segments, buffer_limit):
    
    â•â•â•â•â•â• Circular Buffer with Priority Eviction â•â•â•â•â•â•
    buffer â† CircularBuffer(buffer_limit)
    eviction_heap â† MinHeap(key=lambda s: s.eviction_priority)
    
    FOR segment IN incoming_segments:
        IF buffer.isFull():
            # Evict based on priority
            victim â† selectEvictionVictim(buffer, eviction_heap)
            buffer.remove(victim)
            eviction_heap.remove(victim)
        
        # Insert maintaining playback order
        insertion_pos â† binarySearch(buffer, segment.index)
        buffer.insert(insertion_pos, segment)
        
        # Calculate eviction priority
        priority â† calculateEvictionPriority(segment)
        eviction_heap.push((segment, priority))
    
    FUNCTION calculateEvictionPriority(segment):
        current_time â† getCurrentPlaybackTime()
        
        # Already played segments have lowest priority
        IF segment.index < current_time:
            priority â† segment.index - current_time  # Negative
            
        # Future segments prioritized by proximity
        ELSE:
            distance â† segment.index - current_time
            quality_weight â† segment.quality_level / MAX_QUALITY
            
            # Lower priority = more likely to evict
            priority â† distance Ã— (1 + quality_weight)
        
        RETURN priority
    END FUNCTION
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION calculateQualityScore(segment, viewport):
    # Perceptual quality adjusted for device
    
    # Resolution efficiency
    IF segment.resolution.width > viewport.width:
        resolution_score â† viewport.width / segment.resolution.width
    ELSE:
        resolution_score â† 1.0
    
    # Bitrate efficiency curve (diminishing returns)
    bitrate_score â† LOG(1 + segment.bitrate) / LOG(1 + MAX_BITRATE)
    
    # VMAF perceptual score
    vmaf_score â† segment.vmaf_score / 100
    
    # Codec efficiency bonus
    codec_bonus â† {
        "AV1": 1.3,
        "H.265": 1.15,
        "VP9": 1.1,
        "H.264": 1.0
    }[segment.codec]
    
    RETURN (vmaf_score Ã— 0.5 + 
            bitrate_score Ã— 0.3 + 
            resolution_score Ã— 0.2) Ã— codec_bonus
END FUNCTION

FUNCTION predictBandwidth(window_size):
    # Time-series prediction using exponential smoothing
    history â† getBandwidthHistory()
    alpha â† 0.3  # Smoothing factor
    
    predictions â† []
    current â† history[-1]
    
    FOR i FROM 0 TO window_size-1:
        # Exponential smoothing with trend
        trend â† calculateTrend(history)
        next_value â† alpha Ã— current + (1 - alpha) Ã— (current + trend)
        
        predictions.append(next_value)
        current â† next_value
    
    RETURN predictions
END FUNCTION

â–¶ OPTIMIZATIONS:
  â€¢ Predictive prefetching during high bandwidth
  â€¢ Parallel segment downloading
  â€¢ CDN selection based on latency
  â€¢ Client-side ML for bandwidth prediction
  â€¢ Perceptual quality metrics (VMAF/SSIM)
  â€¢ Device-aware quality selection
```

---

## Search Engine Result Ranking

### ğŸ” Google/Bing Search Result Sorting (Timsort)
**Purpose**: Sorts billions of search results by relevance
**Real Usage**: Google Search, Bing, DuckDuckGo, Elasticsearch

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Timsort with Multi-Factor Ranking & Personalization    â•‘
â•‘ Documents: Billions | Response: <200ms | Factors: 200+            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  documents: array[Document]         # Search result candidates
  ranking_signals: RankingSignals    # 200+ ranking factors
  user_context: UserContext          # Personalization data
  query_intent: QueryIntent          # Navigational/Informational/etc
  experiment_config: ABTest          # A/B test parameters
  
â–¶ OUTPUT:
  ranked_results: array[SearchResult] # Final sorted results
  ranking_explanation: array[Debug]   # Why results ranked this way
  diversity_metrics: DiversityStats   # Result diversity measures

â–¶ DATA STRUCTURES:
  Document: {
    url: string
    title: string
    content: string
    page_rank: float               # Link-based authority
    relevance_score: float         # Query-document similarity
    freshness: datetime            # Last updated
    click_through_rate: float      # Historical CTR
    dwell_time: float              # Average time on page
    signals: map[string, float]    # All ranking signals
  }
  
  RankingSignals: {
    query_dependent: {
      tf_idf: float               # Term frequency
      bm25: float                 # Probabilistic ranking
      semantic_similarity: float   # Embedding distance
      exact_match: boolean
    },
    query_independent: {
      page_authority: float
      domain_authority: float
      spam_score: float
      mobile_friendly: boolean
      https: boolean
      page_speed: float
    },
    user_specific: {
      location_relevance: float
      language_match: float
      personalization_score: float
      device_compatibility: float
    }
  }

â–¶ ALGORITHM PHASE 1: Initial Scoring
FUNCTION computeInitialScores(documents, query, ranking_signals):
    
    â•â•â•â•â•â• Parallel Score Computation â•â•â•â•â•â•
    # Split documents across threads
    num_threads â† getNumCPUs()
    chunk_size â† CEIL(length(documents) / num_threads)
    
    scoring_tasks â† []
    FOR i FROM 0 TO num_threads-1:
        start â† i Ã— chunk_size
        end â† MIN((i + 1) Ã— chunk_size, length(documents))
        chunk â† documents[start:end]
        
        task â† Thread(scoreDocumentChunk, chunk, query, ranking_signals)
        task.start()
        scoring_tasks.append(task)
    
    # Collect scored documents
    scored_documents â† []
    FOR task IN scoring_tasks:
        scored_documents.extend(task.join())
    
    RETURN scored_documents
    
    FUNCTION scoreDocumentChunk(documents, query, signals):
        scored â† []
        
        FOR doc IN documents:
            # Query-dependent features
            doc.signals["relevance"] â† calculateRelevance(doc, query)
            doc.signals["semantic"] â† semanticSimilarity(doc, query)
            
            # Query-independent features  
            doc.signals["authority"] â† doc.page_rank Ã— signals.page_authority_weight
            doc.signals["quality"] â† calculateQualityScore(doc)
            doc.signals["freshness"] â† calculateFreshness(doc)
            
            # User engagement signals
            doc.signals["engagement"] â† (
                doc.click_through_rate Ã— 0.4 +
                normalizedDwellTime(doc.dwell_time) Ã— 0.6
            )
            
            # Initial composite score
            doc.base_score â† combineSignals(doc.signals, signals.weights)
            scored.append(doc)
        
        RETURN scored
    END FUNCTION
END FUNCTION

â–¶ ALGORITHM PHASE 2: Timsort with Custom Comparator
FUNCTION timsortRanking(documents, ranking_function):
    
    MIN_MERGE â† 32
    n â† length(documents)
    
    â•â•â•â•â•â• STEP 1: Find Natural Runs â•â•â•â•â•â•
    runs â† []
    i â† 0
    
    WHILE i < n:
        run_start â† i
        
        IF i == n - 1:
            # Single element run
            runs.append((run_start, i + 1))
            BREAK
        
        # Check if ascending or descending
        IF ranking_function(documents[i], documents[i + 1]) <= 0:
            # Ascending run
            WHILE i + 1 < n AND ranking_function(documents[i], documents[i + 1]) <= 0:
                i += 1
        ELSE:
            # Descending run - will reverse
            WHILE i + 1 < n AND ranking_function(documents[i], documents[i + 1]) > 0:
                i += 1
            # Reverse the run
            reverseRange(documents, run_start, i + 1)
        
        # Extend run to MIN_MERGE if needed
        run_end â† i + 1
        IF run_end - run_start < MIN_MERGE:
            force_end â† MIN(run_start + MIN_MERGE, n)
            binaryInsertionSort(documents, run_start, force_end, ranking_function)
            run_end â† force_end
        
        runs.append((run_start, run_end))
        i â† run_end
    
    â•â•â•â•â•â• STEP 2: Merge Runs â•â•â•â•â•â•
    # Maintain stack of pending runs
    stack â† []
    
    FOR run IN runs:
        stack.append(run)
        mergeCollapse(documents, stack, ranking_function)
    
    mergeForceCollapse(documents, stack, ranking_function)
    
    FUNCTION mergeCollapse(documents, stack, rank_func):
        WHILE length(stack) > 1:
            n â† length(stack)
            
            # Tim Peters' merge rules
            IF (n >= 3 AND stack[n-3][1] - stack[n-3][0] <= 
                stack[n-2][1] - stack[n-2][0] + stack[n-1][1] - stack[n-1][0]):
                # Merge middle run with smaller neighbor
                IF stack[n-3][1] - stack[n-3][0] < stack[n-1][1] - stack[n-1][0]:
                    mergeAt(documents, stack, n - 3, rank_func)
                ELSE:
                    mergeAt(documents, stack, n - 2, rank_func)
                    
            ELSE IF stack[n-2][1] - stack[n-2][0] <= stack[n-1][1] - stack[n-1][0]:
                mergeAt(documents, stack, n - 2, rank_func)
                
            ELSE:
                BREAK
    END FUNCTION
    
    FUNCTION mergeAt(documents, stack, i, rank_func):
        # Merge runs at stack[i] and stack[i+1]
        run1 â† stack[i]
        run2 â† stack[i + 1]
        
        merged â† mergeRuns(
            documents[run1[0]:run1[1]],
            documents[run2[0]:run2[1]],
            rank_func
        )
        
        # Copy back to documents
        FOR j, doc IN enumerate(merged):
            documents[run1[0] + j] â† doc
        
        # Update stack
        stack[i] â† (run1[0], run2[1])
        stack.pop(i + 1)
    END FUNCTION
END FUNCTION

â–¶ ALGORITHM PHASE 3: Result Diversification
FUNCTION diversifyResults(ranked_documents, query_intent, max_results=10):
    
    â•â•â•â•â•â• Maximal Marginal Relevance (MMR) â•â•â•â•â•â•
    diversified â† []
    remaining â† ranked_documents.copy()
    
    # Lambda parameter balances relevance vs diversity
    lambda_param â† determineL ambdaByIntent(query_intent)
    
    WHILE length(diversified) < max_results AND length(remaining) > 0:
        best_doc â† NULL
        best_score â† -INFINITY
        
        FOR doc IN remaining:
            # Relevance to query
            relevance â† doc.base_score
            
            # Similarity to already selected documents
            max_similarity â† 0
            FOR selected IN diversified:
                similarity â† calculateSimilarity(doc, selected)
                max_similarity â† MAX(max_similarity, similarity)
            
            # MMR score
            mmr_score â† lambda_param Ã— relevance - (1 - lambda_param) Ã— max_similarity
            
            IF mmr_score > best_score:
                best_score â† mmr_score
                best_doc â† doc
        
        diversified.append(best_doc)
        remaining.remove(best_doc)
    
    RETURN diversified
    
    FUNCTION calculateSimilarity(doc1, doc2):
        # Multiple similarity dimensions
        
        # Domain diversity
        domain_sim â† 1.0 IF doc1.domain == doc2.domain ELSE 0.0
        
        # Content similarity
        content_sim â† cosineSimilarity(doc1.embedding, doc2.embedding)
        
        # Topic similarity
        topic_sim â† jaccardSimilarity(doc1.topics, doc2.topics)
        
        # Weighted combination
        RETURN (domain_sim Ã— 0.3 + 
                content_sim Ã— 0.5 + 
                topic_sim Ã— 0.2)
    END FUNCTION
END FUNCTION

â–¶ ALGORITHM PHASE 4: Personalization Layer
FUNCTION personalizeRanking(documents, user_context):
    
    â•â•â•â•â•â• User-Specific Re-ranking â•â•â•â•â•â•
    FOR doc IN documents:
        personalization_boost â† 0
        
        # Click history
        IF doc.url IN user_context.clicked_urls:
            recency â† daysSince(user_context.clicked_urls[doc.url])
            personalization_boost += 0.2 Ã— exp(-recency / 30)  # Decay
        
        # Topic preferences
        FOR topic IN doc.topics:
            IF topic IN user_context.interests:
                interest_strength â† user_context.interests[topic]
                personalization_boost += 0.1 Ã— interest_strength
        
        # Location relevance
        IF doc.location_specific:
            distance â† haversineDistance(doc.location, user_context.location)
            location_boost â† 1 / (1 + distance / 50)  # 50km normalization
            personalization_boost += 0.15 Ã— location_boost
        
        # Language preference
        IF doc.language == user_context.preferred_language:
            personalization_boost += 0.1
        
        # Time-based patterns
        hour â† getCurrentHour()
        IF doc.category IN user_context.hourly_preferences[hour]:
            personalization_boost += 0.05
        
        # Apply boost with diminishing returns
        doc.final_score â† doc.base_score Ã— (1 + tanh(personalization_boost))
    
    # Re-sort with personalized scores
    documents â† timsortRanking(documents, lambda a, b: b.final_score - a.final_score)
    
    RETURN documents
END FUNCTION

â–¶ SPECIALIZED: Real-time Index Updates
FUNCTION incrementalIndexSort(index, new_documents, deletions):
    
    â•â•â•â•â•â• Merge-Based Incremental Sorting â•â•â•â•â•â•
    # Remove deleted documents
    FOR doc_id IN deletions:
        index.markDeleted(doc_id)
    
    # Sort new documents
    new_documents â† timsortRanking(new_documents, standardRankingFunction)
    
    # Merge with existing sorted index
    merged_index â† []
    old_iter â† iter(index)
    new_iter â† iter(new_documents)
    
    old_doc â† next(old_iter, NULL)
    new_doc â† next(new_iter, NULL)
    
    WHILE old_doc != NULL OR new_doc != NULL:
        IF old_doc == NULL:
            merged_index.append(new_doc)
            new_doc â† next(new_iter, NULL)
            
        ELSE IF new_doc == NULL:
            IF NOT old_doc.deleted:
                merged_index.append(old_doc)
            old_doc â† next(old_iter, NULL)
            
        ELSE:
            IF old_doc.deleted:
                old_doc â† next(old_iter, NULL)
                CONTINUE
            
            IF standardRankingFunction(old_doc, new_doc) >= 0:
                merged_index.append(old_doc)
                old_doc â† next(old_iter, NULL)
            ELSE:
                merged_index.append(new_doc)
                new_doc â† next(new_iter, NULL)
    
    RETURN merged_index
END FUNCTION

â–¶ PERFORMANCE OPTIMIZATIONS:
  â€¢ SIMD instructions for score computation
  â€¢ GPU acceleration for embedding similarity
  â€¢ Approximate scoring for tail documents
  â€¢ Early termination for top-k retrieval
  â€¢ Cascade ranking (cheap â†’ expensive features)
  â€¢ Index sharding for parallel processing
  â€¢ Result caching with TTL
```