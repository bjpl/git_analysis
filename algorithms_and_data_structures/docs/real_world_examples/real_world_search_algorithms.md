# Real-World Search Algorithms: Comprehensive Examples with Pseudocode

## Table of Contents
1. [Web Search Engine](#web-search-engine)
2. [GPS Navigation](#gps-navigation)
3. [Database Query Optimization](#database-query-optimization)
4. [Image Recognition Search](#image-recognition-search)
5. [Game AI Pathfinding](#game-ai-pathfinding)
6. [File System Search](#file-system-search)
7. [Similarity Search](#similarity-search)
8. [Network Routing](#network-routing)

---

## Web Search Engine

### ğŸ” Google-Style Web Search (PageRank + Inverted Index)
**Purpose**: Ranks and retrieves web pages based on query relevance and authority
**Real Usage**: Google, Bing, DuckDuckGo search engines

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: Web Search with PageRank and TF-IDF Ranking            â•‘
â•‘ Scale: Billions of pages | Response Time: <200ms                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  query: string                      # User search query
  web_graph: Graph[WebPage]          # Link structure of the web
  index: InvertedIndex               # Pre-built search index
  user_context: UserContext          # Location, search history, etc.
  
â–¶ OUTPUT:
  results: array[SearchResult]       # Ranked search results
  snippets: array[TextSnippet]       # Preview text for each result
  suggestions: array[string]         # Related search suggestions

â–¶ DATA STRUCTURES:
  WebPage: {
    url: string
    title: string
    content: string
    links: array[url]              # Outgoing links
    pagerank: float                # Pre-computed PageRank score
    last_crawled: datetime
    domain_authority: float
  }
  
  InvertedIndex: {
    terms: HashMap[string, PostingList]
    documents: HashMap[doc_id, DocumentInfo]
    bigrams: HashMap[pair, array[doc_id]]  # For phrase search
  }
  
  PostingList: {
    doc_frequency: int             # Number of docs containing term
    postings: array[Posting]       # Document occurrences
  }
  
  Posting: {
    doc_id: int
    term_frequency: int            # Occurrences in document
    positions: array[int]          # Word positions in document
    field_weights: map[field, weight]  # title:3.0, body:1.0, etc.
  }

â–¶ ALGORITHM PART 1: PageRank Computation (Offline)
FUNCTION computePageRank(web_graph, damping_factor=0.85, iterations=30):
    
    â•â•â•â•â•â• STEP 1: Initialize PageRank Values â•â•â•â•â•â•
    N â† number_of_pages(web_graph)
    pagerank â† HashMap()
    
    # Start with uniform distribution
    FOR page IN web_graph.nodes():
        pagerank[page] â† 1.0 / N
    
    â•â•â•â•â•â• STEP 2: Build Adjacency Structure â•â•â•â•â•â•
    # Create reverse index for incoming links
    incoming_links â† HashMap()
    outgoing_count â† HashMap()
    
    FOR page IN web_graph.nodes():
        outgoing_count[page] â† length(page.links)
        
        FOR target_url IN page.links:
            IF target_url NOT IN incoming_links:
                incoming_links[target_url] â† []
            incoming_links[target_url].append(page.url)
    
    â•â•â•â•â•â• STEP 3: Iterative PageRank Calculation â•â•â•â•â•â•
    FOR iteration FROM 1 TO iterations:
        new_pagerank â† HashMap()
        
        FOR page IN web_graph.nodes():
            # Random surfer component
            rank â† (1 - damping_factor) / N
            
            # Incoming link contribution
            IF page.url IN incoming_links:
                FOR source_url IN incoming_links[page.url]:
                    source_pr â† pagerank[source_url]
                    out_links â† outgoing_count[source_url]
                    
                    IF out_links > 0:
                        rank += damping_factor Ã— (source_pr / out_links)
            
            new_pagerank[page.url] â† rank
        
        # Check convergence
        delta â† 0
        FOR page IN web_graph.nodes():
            delta += ABS(new_pagerank[page.url] - pagerank[page.url])
        
        pagerank â† new_pagerank
        
        IF delta < 0.0001:  # Convergence threshold
            BREAK
    
    # Store PageRank in pages
    FOR page IN web_graph.nodes():
        page.pagerank â† pagerank[page.url]
    
    RETURN pagerank
END FUNCTION

â–¶ ALGORITHM PART 2: Query Processing and Search
FUNCTION searchWeb(query, index, web_graph, user_context):
    
    â•â•â•â•â•â• STEP 1: Query Analysis and Expansion â•â•â•â•â•â•
    # Tokenize and normalize query
    query_terms â† tokenize(toLowerCase(query))
    query_terms â† removeStopWords(query_terms)
    
    # Spell correction
    corrected_terms â† []
    FOR term IN query_terms:
        IF term NOT IN index.terms:
            correction â† spellCorrect(term, index.terms.keys())
            corrected_terms.append(correction)
        ELSE:
            corrected_terms.append(term)
    
    # Query expansion with synonyms
    expanded_terms â† corrected_terms.copy()
    FOR term IN corrected_terms:
        synonyms â† getSynonyms(term, limit=2)
        expanded_terms.extend(synonyms)
    
    # Detect phrase queries (terms in quotes)
    phrases â† extractPhrases(query)
    
    â•â•â•â•â•â• STEP 2: Retrieve Candidate Documents â•â•â•â•â•â•
    candidate_docs â† Set()
    term_doc_scores â† HashMap()  # doc_id -> term -> score
    
    FOR term IN expanded_terms:
        IF term NOT IN index.terms:
            CONTINUE
        
        posting_list â† index.terms[term]
        idf â† log(index.total_docs / posting_list.doc_frequency)
        
        FOR posting IN posting_list.postings:
            doc_id â† posting.doc_id
            candidate_docs.add(doc_id)
            
            # Calculate TF-IDF score
            tf â† 1 + log(posting.term_frequency)
            
            # Apply field boosts
            field_score â† 0
            FOR field, weight IN posting.field_weights:
                IF field == "title":
                    field_score += weight Ã— 3.0  # Title boost
                ELSE IF field == "url":
                    field_score += weight Ã— 2.0  # URL boost
                ELSE:
                    field_score += weight Ã— 1.0  # Body text
            
            score â† tf Ã— idf Ã— field_score
            
            IF doc_id NOT IN term_doc_scores:
                term_doc_scores[doc_id] â† HashMap()
            term_doc_scores[doc_id][term] â† score
    
    # Handle phrase queries
    FOR phrase IN phrases:
        phrase_docs â† findPhraseDocuments(phrase, index)
        candidate_docs â† candidate_docs.intersection(phrase_docs)
    
    â•â•â•â•â•â• STEP 3: Score and Rank Documents â•â•â•â•â•â•
    doc_scores â† []
    
    FOR doc_id IN candidate_docs:
        doc_info â† index.documents[doc_id]
        
        # BM25 scoring (improvement over TF-IDF)
        bm25_score â† 0
        k1 â† 1.2  # Term frequency saturation
        b â† 0.75  # Length normalization
        
        avg_doc_length â† index.average_doc_length
        doc_length â† doc_info.length
        
        FOR term IN corrected_terms:
            IF term IN term_doc_scores[doc_id]:
                tf â† term_doc_scores[doc_id][term]
                idf â† log((index.total_docs - posting_list.doc_frequency + 0.5) / 
                         (posting_list.doc_frequency + 0.5))
                
                norm_tf â† (tf Ã— (k1 + 1)) / (tf + k1 Ã— (1 - b + b Ã— doc_length / avg_doc_length))
                bm25_score += idf Ã— norm_tf
        
        # Combine with PageRank
        page â† web_graph.getPage(doc_info.url)
        pagerank_score â† page.pagerank Ã— 100  # Scale PageRank
        
        # Additional signals
        freshness_score â† calculateFreshness(page.last_crawled)
        domain_score â† page.domain_authority
        
        # User context personalization
        personalization_score â† 0
        IF doc_info.url IN user_context.visited_urls:
            personalization_score += 0.1
        
        IF doc_info.domain IN user_context.preferred_domains:
            personalization_score += 0.2
        
        # Location relevance
        IF user_context.location:
            location_score â† calculateLocationRelevance(doc_info, user_context.location)
        ELSE:
            location_score â† 0
        
        # Combine all signals
        final_score â† (
            bm25_score Ã— 0.4 +
            pagerank_score Ã— 0.3 +
            domain_score Ã— 0.1 +
            freshness_score Ã— 0.1 +
            personalization_score Ã— 0.05 +
            location_score Ã— 0.05
        )
        
        doc_scores.append((doc_id, final_score))
    
    # Sort by score
    doc_scores â† SORT(doc_scores, key=score, descending=True)
    
    â•â•â•â•â•â• STEP 4: Generate Snippets and Results â•â•â•â•â•â•
    results â† []
    snippets â† []
    
    FOR (doc_id, score) IN doc_scores[:10]:  # Top 10 results
        doc_info â† index.documents[doc_id]
        page â† web_graph.getPage(doc_info.url)
        
        # Generate snippet with query term highlighting
        snippet â† generateSnippet(
            page.content,
            corrected_terms,
            context_window=150  # characters
        )
        
        result â† SearchResult{
            url: page.url,
            title: page.title,
            score: score,
            snippet: snippet,
            cached_date: page.last_crawled
        }
        
        results.append(result)
        snippets.append(snippet)
    
    â•â•â•â•â•â• STEP 5: Generate Search Suggestions â•â•â•â•â•â•
    suggestions â† []
    
    # Related searches based on co-occurrence
    related_terms â† findRelatedTerms(corrected_terms, index)
    FOR term IN related_terms[:5]:
        suggestion â† query + " " + term
        suggestions.append(suggestion)
    
    # Popular refinements from query logs
    refinements â† getPopularRefinements(query, user_context)
    suggestions.extend(refinements[:3])
    
    RETURN results, snippets, suggestions
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION generateSnippet(content, query_terms, context_window):
    # Find best passage containing query terms
    sentences â† splitIntoSentences(content)
    best_score â† -1
    best_snippet â† ""
    
    FOR i FROM 0 TO length(sentences)-1:
        window â† sentences[MAX(0, i-1):MIN(length(sentences), i+2)]
        window_text â† JOIN(window, " ")
        
        # Count query term occurrences
        score â† 0
        FOR term IN query_terms:
            score += countOccurrences(toLowerCase(window_text), term)
        
        # Prefer earlier passages (more relevant)
        position_penalty â† i / length(sentences)
        score â† score Ã— (1 - 0.3 Ã— position_penalty)
        
        IF score > best_score:
            best_score â† score
            best_snippet â† window_text[:context_window]
    
    # Highlight query terms
    FOR term IN query_terms:
        best_snippet â† REPLACE(best_snippet, term, "<b>" + term + "</b>")
    
    RETURN best_snippet + "..."
END FUNCTION

FUNCTION findPhraseDocuments(phrase, index):
    phrase_terms â† tokenize(phrase)
    
    IF length(phrase_terms) < 2:
        RETURN Set()
    
    # Get documents containing all terms
    doc_sets â† []
    FOR term IN phrase_terms:
        IF term IN index.terms:
            docs â† Set(posting.doc_id FOR posting IN index.terms[term].postings)
            doc_sets.append(docs)
    
    # Intersection of all term documents
    common_docs â† doc_sets[0]
    FOR doc_set IN doc_sets[1:]:
        common_docs â† common_docs.intersection(doc_set)
    
    # Verify phrase adjacency
    phrase_docs â† Set()
    FOR doc_id IN common_docs:
        IF verifyPhraseInDocument(doc_id, phrase_terms, index):
            phrase_docs.add(doc_id)
    
    RETURN phrase_docs
END FUNCTION

â–¶ OPTIMIZATION TECHNIQUES:
  â€¢ Shard index across multiple servers
  â€¢ Use bloom filters for quick existence checks
  â€¢ Cache popular query results
  â€¢ Implement early termination for top-k retrieval
  â€¢ Use SIMD instructions for scoring
  â€¢ Compress posting lists with variable-byte encoding
```

---

## GPS Navigation

### ğŸ—ºï¸ Real-Time GPS Navigation (A* with Dynamic Traffic)
**Purpose**: Finds optimal driving routes considering real-time traffic
**Real Usage**: Google Maps, Waze, Apple Maps

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: A* Pathfinding with Traffic and Constraints            â•‘
â•‘ Graph Size: ~50M nodes | Update Rate: 1Hz | Accuracy: 95%         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  start: Coordinates                 # Starting GPS location
  destination: Coordinates           # Target GPS location
  road_network: Graph[RoadSegment]   # Street map graph
  traffic_data: TrafficMap           # Real-time traffic speeds
  preferences: RoutePreferences      # Avoid tolls, highways, etc.
  departure_time: datetime           # For predictive traffic
  
â–¶ OUTPUT:
  route: array[RoadSegment]          # Optimal path
  distance: float                    # Total distance in km
  duration: int                      # Estimated time in seconds
  alternatives: array[Route]         # Alternative routes
  turn_instructions: array[string]   # Navigation instructions

â–¶ DATA STRUCTURES:
  RoadSegment: {
    id: string
    start_node: NodeID
    end_node: NodeID
    length: float                   # Segment length in meters
    speed_limit: int                # Posted speed in km/h
    road_type: enum                 # highway, arterial, residential
    lanes: int
    restrictions: array[string]     # "no_trucks", "toll", etc.
    geometry: array[Coordinates]    # Shape points
  }
  
  Node: {
    id: NodeID
    location: Coordinates
    edges: array[RoadSegment]       # Outgoing road segments
    traffic_lights: boolean
    elevation: float                # For fuel efficiency
  }
  
  TrafficData: {
    segment_id: string
    current_speed: float            # Current average speed
    historical_speed: array[float]  # By hour of day
    incidents: array[Incident]      # Accidents, construction
    congestion_level: int           # 0-10 scale
  }
  
  SearchNode: {
    node_id: NodeID
    g_cost: float                   # Cost from start
    h_cost: float                   # Heuristic to goal
    f_cost: float                   # g + h
    parent: SearchNode
    arrival_time: datetime
  }

â–¶ ALGORITHM:
FUNCTION findRoute(start, destination, road_network, traffic_data, preferences, departure_time):
    
    â•â•â•â•â•â• STEP 1: Map Matching - Snap GPS to Road â•â•â•â•â•â•
    start_node â† findNearestRoadNode(start, road_network)
    dest_node â† findNearestRoadNode(destination, road_network)
    
    IF start_node == NULL OR dest_node == NULL:
        RETURN ERROR("Location not accessible by road")
    
    â•â•â•â•â•â• STEP 2: Preprocess Graph Based on Constraints â•â•â•â•â•â•
    # Build filtered graph excluding restricted roads
    valid_segments â† []
    
    FOR segment IN road_network.segments:
        # Check user preferences
        IF preferences.avoid_tolls AND "toll" IN segment.restrictions:
            CONTINUE
        
        IF preferences.avoid_highways AND segment.road_type == "highway":
            CONTINUE
        
        IF preferences.vehicle_type == "truck":
            IF "no_trucks" IN segment.restrictions:
                CONTINUE
            IF segment.height_limit < preferences.vehicle_height:
                CONTINUE
        
        valid_segments.append(segment)
    
    â•â•â•â•â•â• STEP 3: Hierarchical A* Search â•â•â•â•â•â•
    # Use contraction hierarchies for long-distance routing
    IF distance(start, destination) > 50:  # km
        contracted_graph â† getContractedGraph(road_network)
        USE contracted_graph FOR initial_search
    
    # Initialize A* data structures
    open_set â† PriorityQueue()  # Min-heap by f_cost
    closed_set â† Set()
    node_costs â† HashMap()      # Best known g_cost for each node
    
    # Create start node
    start_search_node â† SearchNode{
        node_id: start_node,
        g_cost: 0,
        h_cost: haversineDistance(start_node.location, dest_node.location),
        f_cost: h_cost,
        parent: NULL,
        arrival_time: departure_time
    }
    
    open_set.push(start_search_node)
    node_costs[start_node] â† 0
    
    â•â•â•â•â•â• STEP 4: A* Main Loop with Traffic â•â•â•â•â•â•
    WHILE NOT open_set.isEmpty():
        current â† open_set.pop()  # Node with lowest f_cost
        
        # Goal test
        IF current.node_id == dest_node:
            RETURN reconstructPath(current)
        
        closed_set.add(current.node_id)
        
        # Expand neighbors
        FOR edge IN road_network.getOutgoingEdges(current.node_id):
            neighbor_id â† edge.end_node
            
            IF neighbor_id IN closed_set:
                CONTINUE
            
            # Calculate edge cost with real-time traffic
            edge_cost â† calculateEdgeCost(
                edge,
                traffic_data,
                current.arrival_time,
                preferences
            )
            
            tentative_g â† current.g_cost + edge_cost
            
            # Check if we found a better path
            IF neighbor_id IN node_costs:
                IF tentative_g >= node_costs[neighbor_id]:
                    CONTINUE  # Not a better path
            
            node_costs[neighbor_id] â† tentative_g
            
            # Calculate arrival time at neighbor
            travel_time â† calculateTravelTime(edge, traffic_data, current.arrival_time)
            arrival_time â† current.arrival_time + travel_time
            
            # Advanced heuristic considering traffic patterns
            h_cost â† calculateHeuristic(
                neighbor_id,
                dest_node,
                arrival_time,
                traffic_data
            )
            
            neighbor_node â† SearchNode{
                node_id: neighbor_id,
                g_cost: tentative_g,
                h_cost: h_cost,
                f_cost: tentative_g + h_cost,
                parent: current,
                arrival_time: arrival_time
            }
            
            open_set.push(neighbor_node)
        
        # Timeout check for real-time responsiveness
        IF elapsed_time() > 1000:  # milliseconds
            RETURN getBestPartialPath(open_set, dest_node)
    
    RETURN NULL  # No path found
    
    â•â•â•â•â•â• STEP 5: Path Reconstruction and Instructions â•â•â•â•â•â•
    FUNCTION reconstructPath(goal_node):
        path â† []
        current â† goal_node
        total_distance â† 0
        total_duration â† 0
        
        WHILE current.parent != NULL:
            parent â† current.parent
            segment â† road_network.getSegment(parent.node_id, current.node_id)
            path.prepend(segment)
            
            total_distance += segment.length
            travel_time â† current.arrival_time - parent.arrival_time
            total_duration += travel_time
            
            current â† parent
        
        # Generate turn-by-turn instructions
        instructions â† generateInstructions(path)
        
        # Find alternative routes
        alternatives â† findAlternativeRoutes(
            start_node,
            dest_node,
            path,
            road_network,
            traffic_data
        )
        
        RETURN Route{
            segments: path,
            distance: total_distance / 1000,  # Convert to km
            duration: total_duration,
            instructions: instructions,
            alternatives: alternatives
        }
    END FUNCTION
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION calculateEdgeCost(edge, traffic_data, current_time, preferences):
    base_cost â† edge.length  # Distance in meters
    
    # Get current traffic speed
    traffic â† traffic_data.getSegmentTraffic(edge.id)
    
    IF traffic != NULL:
        current_speed â† traffic.current_speed
        
        # Predict future traffic when we'll reach this segment
        IF preferences.use_predictive:
            predicted_speed â† predictTrafficSpeed(
                traffic,
                current_time,
                edge.id
            )
            current_speed â† predicted_speed
    ELSE:
        # Use default speed for road type
        current_speed â† getDefaultSpeed(edge.road_type)
    
    # Time-based cost
    time_cost â† edge.length / (current_speed / 3.6)  # Convert km/h to m/s
    
    # Apply preference weights
    cost â† 0
    
    IF preferences.optimize_for == "time":
        cost â† time_cost
    ELSE IF preferences.optimize_for == "distance":
        cost â† base_cost
    ELSE IF preferences.optimize_for == "fuel":
        # Consider elevation changes and stop-and-go traffic
        fuel_factor â† calculateFuelEfficiency(
            edge,
            current_speed,
            traffic.congestion_level
        )
        cost â† base_cost Ã— fuel_factor
    
    # Penalties for undesirable road features
    IF edge.road_type == "residential" AND preferences.prefer_highways:
        cost *= 1.5
    
    IF "construction" IN traffic.incidents:
        cost *= 2.0
    
    IF edge.hasTrafficLight():
        cost += 30  # Average traffic light delay in seconds
    
    RETURN cost
END FUNCTION

FUNCTION calculateHeuristic(node, goal, arrival_time, traffic_data):
    # Haversine distance as base heuristic
    straight_distance â† haversineDistance(node.location, goal.location)
    
    # Estimate speed based on distance to goal
    IF straight_distance > 10000:  # Over 10km - likely highway
        estimated_speed â† 80  # km/h
    ELSE IF straight_distance > 2000:  # 2-10km - arterial roads
        estimated_speed â† 50
    ELSE:  # Under 2km - local streets
        estimated_speed â† 30
    
    # Adjust for time of day traffic patterns
    hour â† arrival_time.hour
    IF hour IN [7, 8, 17, 18]:  # Rush hour
        estimated_speed *= 0.6
    ELSE IF hour IN [9, 10, 15, 16]:  # Moderate traffic
        estimated_speed *= 0.8
    
    # Convert to time estimate
    time_estimate â† straight_distance / (estimated_speed / 3.6)
    
    RETURN time_estimate
END FUNCTION

FUNCTION generateInstructions(path):
    instructions â† []
    
    FOR i FROM 0 TO length(path)-1:
        segment â† path[i]
        
        IF i == 0:
            instructions.append(f"Head {getCardinalDirection(segment)} on {segment.name}")
        ELSE:
            prev_segment â† path[i-1]
            angle â† calculateTurnAngle(prev_segment, segment)
            
            IF ABS(angle) < 20:
                instruction â† f"Continue on {segment.name}"
            ELSE IF angle > 0:
                IF ABS(angle) > 135:
                    instruction â† f"Make a U-turn onto {segment.name}"
                ELSE IF ABS(angle) > 60:
                    instruction â† f"Turn right onto {segment.name}"
                ELSE:
                    instruction â† f"Bear right onto {segment.name}"
            ELSE:
                IF ABS(angle) > 135:
                    instruction â† f"Make a U-turn onto {segment.name}"
                ELSE IF ABS(angle) > 60:
                    instruction â† f"Turn left onto {segment.name}"
                ELSE:
                    instruction â† f"Bear left onto {segment.name}"
            
            # Add distance
            instruction += f" and continue for {segment.length}m"
            instructions.append(instruction)
    
    instructions.append(f"Arrive at destination")
    
    RETURN instructions
END FUNCTION

FUNCTION findAlternativeRoutes(start, dest, primary_route, network, traffic):
    alternatives â† []
    
    # Use penalty method - increase cost of primary route segments
    FOR segment IN primary_route:
        segment.temp_penalty â† segment.cost Ã— 1.5
    
    # Find up to 2 alternatives
    FOR i FROM 0 TO 1:
        alt_route â† findRoute(start, dest, network, traffic, preferences, departure_time)
        
        IF alt_route != NULL AND alt_route != primary_route:
            # Check if sufficiently different (share < 70% of segments)
            overlap â† calculateOverlap(primary_route, alt_route)
            IF overlap < 0.7:
                alternatives.append(alt_route)
                
                # Penalize this alternative for next iteration
                FOR segment IN alt_route:
                    segment.temp_penalty â† segment.cost Ã— 1.3
    
    # Remove temporary penalties
    FOR segment IN network.segments:
        segment.temp_penalty â† 0
    
    RETURN alternatives
END FUNCTION

â–¶ OPTIMIZATIONS:
  â€¢ Bidirectional search for faster convergence
  â€¢ Contraction hierarchies for long-distance routing
  â€¢ Arc flags for region-based pruning
  â€¢ Precomputed shortest path hints
  â€¢ Partial path caching for common routes
  â€¢ Parallel search for alternatives
```

---

## Database Query Optimization

### ğŸ’¾ SQL Query Execution (B+ Tree Index Search)
**Purpose**: Efficiently retrieves data from massive databases
**Real Usage**: PostgreSQL, MySQL, Oracle, SQL Server

```pseudocode
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ALGORITHM: B+ Tree Index Search with Query Optimization           â•‘
â•‘ Scale: Billions of rows | Response: <10ms | Memory: O(log n)      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ INPUT PARAMETERS:
  query: SQLQuery                    # Parsed SQL SELECT statement
  table: Table                       # Database table
  indexes: array[BPlusTree]          # Available indexes
  statistics: TableStatistics        # Cardinality, selectivity info
  buffer_pool: BufferPool            # Page cache
  
â–¶ OUTPUT:
  result_set: array[Row]             # Query results
  execution_plan: QueryPlan          # Optimized execution plan
  statistics: QueryStats             # Rows examined, time taken

â–¶ DATA STRUCTURES:
  BPlusTree: {
    root: BPlusNode
    order: int                      # Maximum children per node
    key_type: DataType
    leaf_level: int                 # Height of tree
    num_keys: long                  # Total keys in index
  }
  
  BPlusNode: {
    is_leaf: boolean
    keys: array[KeyValue]           # Sorted keys
    children: array[BPlusNode]      # Child pointers (internal nodes)
    records: array[RowPointer]      # Data pointers (leaf nodes)
    next_leaf: BPlusNode            # Next leaf for range scans
    parent: BPlusNode
  }
  
  QueryPlan: {
    operation: enum                 # INDEX_SCAN, TABLE_SCAN, NESTED_LOOP, etc.
    cost: float                     # Estimated cost
    rows: long                      # Estimated rows
    index_used: string
    filter_predicates: array[Predicate]
  }

â–¶ ALGORITHM PART 1: Query Optimization
FUNCTION optimizeQuery(query, table, indexes, statistics):
    
    â•â•â•â•â•â• STEP 1: Parse and Analyze Predicates â•â•â•â•â•â•
    predicates â† extractPredicates(query.WHERE)
    join_conditions â† extractJoins(query.FROM)
    
    # Classify predicates
    equality_predicates â† []
    range_predicates â† []
    complex_predicates â† []
    
    FOR predicate IN predicates:
        IF predicate.operator == "=":
            equality_predicates.append(predicate)
        ELSE IF predicate.operator IN ["<", ">", "<=", ">=", "BETWEEN"]:
            range_predicates.append(predicate)
        ELSE:
            complex_predicates.append(predicate)
    
    â•â•â•â•â•â• STEP 2: Index Selection â•â•â•â•â•â•
    candidate_plans â† []
    
    # Consider each available index
    FOR index IN indexes:
        IF canUseIndex(index, predicates):
            plan â† createIndexPlan(index, predicates, statistics)
            candidate_plans.append(plan)
    
    # Consider full table scan
    table_scan_plan â† createTableScanPlan(table, predicates, statistics)
    candidate_plans.append(table_scan_plan)
    
    # Consider composite index usage
    FOR index IN indexes:
        IF index.is_composite:
            # Check if we can use index for sorting
            IF matchesOrderBy(index, query.ORDER_BY):
                plan â† createIndexOrderPlan(index, predicates, statistics)
                plan.cost *= 0.8  # Prefer index that avoids sorting
                candidate_plans.append(plan)
    
    â•â•â•â•â•â• STEP 3: Cost Estimation â•â•â•â•â•â•
    FOR plan IN candidate_plans:
        # I/O cost estimation
        if plan.operation == "INDEX_SCAN":
            index_height â† plan.index.leaf_level
            index_selectivity â† estimateSelectivity(predicates, statistics)
            
            # Cost = tree traversal + leaf scan + data fetch
            tree_cost â† index_height  # Page reads to reach leaf
            leaf_cost â† CEIL(plan.index.num_keys Ã— index_selectivity / keys_per_page)
            data_cost â† plan.rows  # Assume random I/O for data pages
            
            plan.cost â† tree_cost + leaf_cost + data_cost Ã— random_io_factor
            
        ELSE IF plan.operation == "TABLE_SCAN":
            pages â† table.num_pages
            plan.cost â† pages Ã— sequential_io_factor
            
            # Apply filter selectivity
            selectivity â† estimateSelectivity(predicates, statistics)
            plan.rows â† table.num_rows Ã— selectivity
    
    # Select best plan
    best_plan â† MIN(candidate_plans, key=lambda p: p.cost)
    
    RETURN best_plan
END FUNCTION

â–¶ ALGORITHM PART 2: B+ Tree Search Execution
FUNCTION executeIndexSearch(index, search_key, operation, buffer_pool):
    
    â•â•â•â•â•â• STEP 1: Tree Traversal to Leaf â•â•â•â•â•â•
    current â† index.root
    path â† []  # For potential node splits
    
    WHILE NOT current.is_leaf:
        path.append(current)
        
        # Binary search within node
        child_index â† binarySearchNode(current, search_key, operation)
        
        # Pin page in buffer pool
        child_page_id â† current.children[child_index].page_id
        child_page â† buffer_pool.getPage(child_page_id)
        
        IF child_page == NULL:
            # Page not in memory, fetch from disk
            child_page â† readPageFromDisk(child_page_id)
            buffer_pool.addPage(child_page)
        
        current â† deserializeNode(child_page)
    
    â•â•â•â•â•â• STEP 2: Leaf Level Search â•â•â•â•â•â•
    IF operation == "EQUAL":
        # Point query
        key_index â† binarySearchLeaf(current, search_key)
        
        IF key_index >= 0 AND current.keys[key_index] == search_key:
            RETURN [current.records[key_index]]
        ELSE:
            RETURN []  # Not found
    
    ELSE IF operation == "RANGE":
        # Range query
        results â† []
        start_key, end_key â† search_key  # Tuple of (min, max)
        
        # Find starting position
        start_index â† binarySearchLeaf(current, start_key)
        IF start_index < 0:
            start_index â† -(start_index + 1)  # Insertion point
        
        # Scan leaf nodes
        WHILE current != NULL:
            FOR i FROM start_index TO length(current.keys)-1:
                IF current.keys[i] > end_key:
                    RETURN results  # Exceeded range
                
                IF current.keys[i] >= start_key:
                    results.append(current.records[i])
            
            # Move to next leaf
            current â† current.next_leaf
            start_index â† 0  # Start from beginning of next leaf
            
            # Check for early termination
            IF current != NULL AND current.keys[0] > end_key:
                BREAK
        
        RETURN results
    
    ELSE IF operation IN ["GREATER", "GREATER_EQUAL", "LESS", "LESS_EQUAL"]:
        # Half-range query
        results â† []
        
        IF operation IN ["GREATER", "GREATER_EQUAL"]:
            # Scan forward from search_key
            start_index â† binarySearchLeaf(current, search_key)
            IF start_index < 0:
                start_index â† -(start_index + 1)
            
            IF operation == "GREATER":
                start_index += 1  # Exclude equal values
            
            WHILE current != NULL:
                FOR i FROM start_index TO length(current.keys)-1:
                    results.append(current.records[i])
                
                current â† current.next_leaf
                start_index â† 0
        
        ELSE:  # LESS or LESS_EQUAL
            # Scan from beginning up to search_key
            current â† findLeftmostLeaf(index.root)
            
            WHILE current != NULL:
                FOR i FROM 0 TO length(current.keys)-1:
                    IF operation == "LESS" AND current.keys[i] >= search_key:
                        RETURN results
                    IF operation == "LESS_EQUAL" AND current.keys[i] > search_key:
                        RETURN results
                    
                    results.append(current.records[i])
                
                current â† current.next_leaf
        
        RETURN results
END FUNCTION

â–¶ HELPER FUNCTIONS:
FUNCTION binarySearchNode(node, key, operation):
    # Binary search for child pointer in internal node
    left â† 0
    right â† length(node.keys) - 1
    
    WHILE left <= right:
        mid â† (left + right) / 2
        
        IF key == node.keys[mid]:
            IF operation IN ["GREATER", "GREATER_EQUAL"]:
                RETURN mid + 1  # Go right for greater values
            ELSE:
                RETURN mid  # Go left for less/equal values
        
        ELSE IF key < node.keys[mid]:
            right â† mid - 1
        ELSE:
            left â† mid + 1
    
    RETURN left  # Child index for key insertion point
END FUNCTION

FUNCTION binarySearchLeaf(leaf, key):
    # Binary search in leaf node
    left â† 0
    right â† length(leaf.keys) - 1
    
    WHILE left <= right:
        mid â† (left + right) / 2
        
        IF key == leaf.keys[mid]:
            RETURN mid
        ELSE IF key < leaf.keys[mid]:
            right â† mid - 1
        ELSE:
            left â† mid + 1
    
    RETURN -(left + 1)  # Negative indicates insertion point
END FUNCTION

â–¶ ALGORITHM PART 3: Join Processing
FUNCTION executeJoin(left_table, right_table, join_condition, indexes):
    
    â•â•â•â•â•â• Choose Join Algorithm â•â•â•â•â•â•
    left_size â† left_table.num_rows
    right_size â† right_table.num_rows
    
    # Check for index on join column
    right_index â† findIndex(indexes, right_table, join_condition.right_column)
    left_index â† findIndex(indexes, left_table, join_condition.left_column)
    
    IF right_index != NULL AND left_size < right_size:
        RETURN indexNestedLoopJoin(left_table, right_table, join_condition, right_index)
    ELSE IF left_size + right_size < available_memory:
        RETURN hashJoin(left_table, right_table, join_condition)
    ELSE:
        RETURN sortMergeJoin(left_table, right_table, join_condition)
END FUNCTION

FUNCTION indexNestedLoopJoin(outer, inner, condition, index):
    results â† []
    
    FOR outer_row IN outer.scan():
        join_key â† outer_row[condition.left_column]
        
        # Use index to find matching inner rows
        matching_rows â† executeIndexSearch(index, join_key, "EQUAL", buffer_pool)
        
        FOR inner_row IN matching_rows:
            IF evaluateJoinCondition(outer_row, inner_row, condition):
                combined_row â† concatenate(outer_row, inner_row)
                results.append(combined_row)
    
    RETURN results
END FUNCTION

FUNCTION hashJoin(left, right, condition):
    # Build phase
    hash_table â† HashMap()
    
    FOR row IN right.scan():  # Build on smaller table
        key â† row[condition.right_column]
        IF key NOT IN hash_table:
            hash_table[key] â† []
        hash_table[key].append(row)
    
    # Probe phase
    results â† []
    
    FOR row IN left.scan():
        key â† row[condition.left_column]
        IF key IN hash_table:
            FOR matching_row IN hash_table[key]:
                combined â† concatenate(row, matching_row)
                results.append(combined)
    
    RETURN results
END FUNCTION

â–¶ OPTIMIZATION TECHNIQUES:
  â€¢ Index-only scans (covering indexes)
  â€¢ Bitmap indexes for low-cardinality columns
  â€¢ Partition pruning for large tables
  â€¢ Parallel query execution
  â€¢ Adaptive query optimization
  â€¢ Join order optimization
  â€¢ Predicate pushdown
  â€¢ Lazy evaluation for LIMIT queries
```