# Real-World Search Algorithms: Part 2

## Image Recognition Search

### üì∑ Visual Search Engine (KD-Tree for Feature Matching)
**Purpose**: Finds similar images based on visual features
**Real Usage**: Google Lens, Pinterest Visual Search, TinEye

```pseudocode
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë ALGORITHM: Image Similarity Search with KD-Trees and SIFT         ‚ïë
‚ïë Database: 100M+ images | Query Time: <500ms | Accuracy: 92%       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñ∂ INPUT PARAMETERS:
  query_image: Image                 # Input image to search
  image_database: array[ImageRecord] # Pre-indexed image collection
  kd_tree: KDTree                   # Spatial index of features
  similarity_threshold: float        # Minimum similarity score
  max_results: int                   # Number of results to return
  
‚ñ∂ OUTPUT:
  matches: array[ImageMatch]         # Similar images found
  bounding_boxes: array[Rectangle]   # Object locations in matches
  confidence_scores: array[float]    # Match confidence levels

‚ñ∂ DATA STRUCTURES:
  Image: {
    pixels: array[array[RGB]]        # Raw pixel data
    width: int
    height: int
    metadata: ImageMetadata
  }
  
  ImageRecord: {
    id: string
    features: array[FeatureVector]   # SIFT/SURF/ORB descriptors
    global_descriptor: array[float]  # Color histogram, GIST, etc.
    thumbnail: Image
    source_url: string
  }
  
  FeatureVector: {
    keypoint: Point2D                # x, y location
    scale: float                     # Feature scale
    orientation: float               # Dominant orientation
    descriptor: array[float]         # 128D SIFT descriptor
  }
  
  KDNode: {
    point: FeatureVector
    left: KDNode
    right: KDNode
    split_dim: int                   # Dimension to split on
    image_id: string                 # Source image reference
  }

‚ñ∂ ALGORITHM PART 1: Feature Extraction
FUNCTION extractImageFeatures(image):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Build Gaussian Scale Space ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Create multi-scale representation
    num_octaves ‚Üê 4
    scales_per_octave ‚Üê 5
    initial_sigma ‚Üê 1.6
    
    scale_space ‚Üê []
    
    FOR octave FROM 0 TO num_octaves-1:
        octave_images ‚Üê []
        base_image ‚Üê downsample(image, 2^octave)
        
        FOR scale FROM 0 TO scales_per_octave-1:
            sigma ‚Üê initial_sigma √ó 2^(octave + scale/scales_per_octave)
            blurred ‚Üê gaussianBlur(base_image, sigma)
            octave_images.append(blurred)
        
        scale_space.append(octave_images)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Detect Keypoints (SIFT) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    keypoints ‚Üê []
    
    # Compute Difference of Gaussians (DoG)
    FOR octave FROM 0 TO num_octaves-1:
        FOR scale FROM 1 TO scales_per_octave-2:
            # DoG approximates Laplacian of Gaussian
            dog ‚Üê scale_space[octave][scale+1] - scale_space[octave][scale]
            
            # Find local extrema
            FOR y FROM 1 TO height(dog)-2:
                FOR x FROM 1 TO width(dog)-2:
                    pixel ‚Üê dog[y][x]
                    
                    # Check if extremum in 3x3x3 neighborhood
                    is_extremum ‚Üê TRUE
                    
                    # Check spatial neighbors
                    FOR dy FROM -1 TO 1:
                        FOR dx FROM -1 TO 1:
                            IF dy == 0 AND dx == 0:
                                CONTINUE
                            
                            neighbor ‚Üê dog[y+dy][x+dx]
                            IF (pixel > 0 AND neighbor >= pixel) OR
                               (pixel < 0 AND neighbor <= pixel):
                                is_extremum ‚Üê FALSE
                                BREAK
                    
                    # Check scale neighbors
                    IF is_extremum:
                        above ‚Üê scale_space[octave][scale+2][y][x]
                        below ‚Üê scale_space[octave][scale][y][x]
                        
                        IF (pixel > 0 AND (above >= pixel OR below >= pixel)) OR
                           (pixel < 0 AND (above <= pixel OR below <= pixel)):
                            is_extremum ‚Üê FALSE
                    
                    IF is_extremum AND ABS(pixel) > 0.03:  # Threshold
                        keypoint ‚Üê refineKeypoint(x, y, octave, scale, dog)
                        IF keypoint != NULL:
                            keypoints.append(keypoint)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: Compute Descriptors ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    features ‚Üê []
    
    FOR keypoint IN keypoints:
        # Compute gradient magnitude and orientation
        image_patch ‚Üê getImagePatch(
            scale_space[keypoint.octave][keypoint.scale],
            keypoint.x,
            keypoint.y,
            16  # 16x16 patch
        )
        
        # Rotate patch to canonical orientation
        rotated_patch ‚Üê rotatePatch(image_patch, -keypoint.orientation)
        
        # Build 128D descriptor (4x4 grid of 8-bin histograms)
        descriptor ‚Üê array[128]
        descriptor_index ‚Üê 0
        
        FOR grid_y FROM 0 TO 3:
            FOR grid_x FROM 0 TO 3:
                # Get 4x4 sub-patch
                sub_patch ‚Üê rotated_patch[grid_y*4:(grid_y+1)*4, grid_x*4:(grid_x+1)*4]
                
                # Compute gradient histogram
                histogram ‚Üê array[8]  # 8 orientation bins
                
                FOR y FROM 0 TO 3:
                    FOR x FROM 0 TO 3:
                        dx ‚Üê sub_patch[y][x+1] - sub_patch[y][x-1]
                        dy ‚Üê sub_patch[y+1][x] - sub_patch[y-1][x]
                        
                        magnitude ‚Üê sqrt(dx¬≤ + dy¬≤)
                        orientation ‚Üê atan2(dy, dx)
                        
                        # Add to histogram with Gaussian weighting
                        bin ‚Üê FLOOR(orientation / (2œÄ/8))
                        weight ‚Üê magnitude √ó gaussian(x-1.5, y-1.5, 1.5)
                        histogram[bin] += weight
                
                # Add to descriptor
                FOR bin FROM 0 TO 7:
                    descriptor[descriptor_index] ‚Üê histogram[bin]
                    descriptor_index += 1
        
        # Normalize descriptor
        norm ‚Üê sqrt(SUM(descriptor[i]¬≤ for i in 0..127))
        FOR i FROM 0 TO 127:
            descriptor[i] ‚Üê descriptor[i] / norm
            # Threshold to reduce illumination effects
            descriptor[i] ‚Üê MIN(descriptor[i], 0.2)
        
        # Renormalize
        norm ‚Üê sqrt(SUM(descriptor[i]¬≤ for i in 0..127))
        FOR i FROM 0 TO 127:
            descriptor[i] ‚Üê descriptor[i] / norm
        
        features.append(FeatureVector{
            keypoint: Point2D(keypoint.x, keypoint.y),
            scale: keypoint.scale,
            orientation: keypoint.orientation,
            descriptor: descriptor
        })
    
    RETURN features
END FUNCTION

‚ñ∂ ALGORITHM PART 2: KD-Tree Construction (Offline)
FUNCTION buildKDTree(all_features, max_depth=40):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Build Balanced KD-Tree ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    FUNCTION buildNode(features, depth):
        IF length(features) == 0:
            RETURN NULL
        
        IF length(features) == 1:
            RETURN KDNode{
                point: features[0],
                left: NULL,
                right: NULL,
                split_dim: -1,
                image_id: features[0].image_id
            }
        
        # Choose splitting dimension (cycle through dimensions)
        k ‚Üê 128  # Dimensionality of SIFT descriptors
        split_dim ‚Üê depth % k
        
        # For high dimensions, use dimension with highest variance
        IF depth < 10:  # Only for top levels
            variances ‚Üê []
            FOR dim FROM 0 TO k-1:
                values ‚Üê [f.descriptor[dim] FOR f IN features]
                variance ‚Üê calculateVariance(values)
                variances.append(variance)
            split_dim ‚Üê argmax(variances)
        
        # Sort by splitting dimension
        features ‚Üê SORT(features, key=lambda f: f.descriptor[split_dim])
        
        # Find median
        median_idx ‚Üê length(features) / 2
        median_feature ‚Üê features[median_idx]
        
        # Recursively build subtrees
        left_features ‚Üê features[0:median_idx]
        right_features ‚Üê features[median_idx+1:]
        
        node ‚Üê KDNode{
            point: median_feature,
            split_dim: split_dim,
            image_id: median_feature.image_id,
            left: buildNode(left_features, depth + 1),
            right: buildNode(right_features, depth + 1)
        }
        
        RETURN node
    END FUNCTION
    
    RETURN buildNode(all_features, 0)
END FUNCTION

‚ñ∂ ALGORITHM PART 3: Image Search
FUNCTION searchSimilarImages(query_image, kd_tree, database, similarity_threshold, max_results):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Extract Query Features ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    query_features ‚Üê extractImageFeatures(query_image)
    
    IF length(query_features) == 0:
        RETURN []  # No features found
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Find Nearest Neighbors for Each Feature ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # For each query feature, find k nearest neighbors
    k_neighbors ‚Üê 2  # Ratio test requires 2 nearest
    feature_matches ‚Üê []
    
    FOR query_feature IN query_features:
        neighbors ‚Üê kdTreeKNN(kd_tree, query_feature, k_neighbors)
        
        IF length(neighbors) >= 2:
            # Lowe's ratio test - filter ambiguous matches
            best_distance ‚Üê euclideanDistance(query_feature.descriptor, neighbors[0].descriptor)
            second_distance ‚Üê euclideanDistance(query_feature.descriptor, neighbors[1].descriptor)
            
            IF best_distance < 0.7 √ó second_distance:  # Good match
                feature_matches.append({
                    query: query_feature,
                    match: neighbors[0],
                    distance: best_distance,
                    image_id: neighbors[0].image_id
                })
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: Aggregate Matches by Image ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    image_votes ‚Üê HashMap()  # image_id -> array[matches]
    
    FOR match IN feature_matches:
        IF match.image_id NOT IN image_votes:
            image_votes[match.image_id] ‚Üê []
        image_votes[match.image_id].append(match)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 4: Geometric Verification (RANSAC) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    verified_matches ‚Üê []
    
    FOR image_id, matches IN image_votes.items():
        IF length(matches) < 4:  # Need at least 4 for homography
            CONTINUE
        
        # Extract point correspondences
        src_points ‚Üê [m.query.keypoint FOR m IN matches]
        dst_points ‚Üê [m.match.keypoint FOR m IN matches]
        
        # RANSAC to find homography
        best_inliers ‚Üê []
        best_homography ‚Üê NULL
        max_iterations ‚Üê 1000
        
        FOR iteration FROM 0 TO max_iterations:
            # Random sample of 4 matches
            sample_indices ‚Üê randomSample(range(length(matches)), 4)
            sample_src ‚Üê [src_points[i] FOR i IN sample_indices]
            sample_dst ‚Üê [dst_points[i] FOR i IN sample_indices]
            
            # Compute homography from 4 points
            H ‚Üê computeHomography(sample_src, sample_dst)
            
            IF H == NULL:
                CONTINUE
            
            # Count inliers
            inliers ‚Üê []
            FOR i FROM 0 TO length(matches)-1:
                projected ‚Üê H √ó src_points[i]
                error ‚Üê euclideanDistance(projected, dst_points[i])
                
                IF error < 5.0:  # Pixel threshold
                    inliers.append(i)
            
            IF length(inliers) > length(best_inliers):
                best_inliers ‚Üê inliers
                best_homography ‚Üê H
            
            # Early termination if good enough
            IF length(best_inliers) > 0.8 √ó length(matches):
                BREAK
        
        # Keep only geometrically consistent matches
        IF length(best_inliers) >= 10:
            verified_matches.append({
                image_id: image_id,
                num_matches: length(best_inliers),
                homography: best_homography,
                inlier_matches: [matches[i] FOR i IN best_inliers]
            })
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 5: Rank Results ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    results ‚Üê []
    
    FOR match IN verified_matches:
        # Compute similarity score
        score ‚Üê 0.0
        
        # Feature match score
        feature_score ‚Üê match.num_matches / length(query_features)
        feature_score ‚Üê MIN(feature_score, 1.0)
        
        # Geometric consistency score
        geometric_score ‚Üê calculateHomographyQuality(match.homography)
        
        # Global descriptor similarity (color histogram, etc.)
        db_image ‚Üê database.getImage(match.image_id)
        global_sim ‚Üê compareGlobalDescriptors(
            query_image.global_descriptor,
            db_image.global_descriptor
        )
        
        # Combined score
        score ‚Üê (feature_score √ó 0.5 + 
                geometric_score √ó 0.3 + 
                global_sim √ó 0.2)
        
        IF score >= similarity_threshold:
            results.append(ImageMatch{
                image_id: match.image_id,
                score: score,
                matched_features: match.num_matches,
                homography: match.homography
            })
    
    # Sort by score
    results ‚Üê SORT(results, key=score, descending=True)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 6: Generate Bounding Boxes ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    bounding_boxes ‚Üê []
    
    FOR result IN results[:max_results]:
        # Transform query image corners using homography
        h, w ‚Üê query_image.height, query_image.width
        corners ‚Üê [
            Point2D(0, 0),
            Point2D(w, 0),
            Point2D(w, h),
            Point2D(0, h)
        ]
        
        transformed_corners ‚Üê []
        FOR corner IN corners:
            transformed ‚Üê result.homography √ó corner
            transformed_corners.append(transformed)
        
        bbox ‚Üê boundingBox(transformed_corners)
        bounding_boxes.append(bbox)
    
    RETURN results[:max_results], bounding_boxes
END FUNCTION

‚ñ∂ HELPER FUNCTION: KD-Tree K-Nearest Neighbors
FUNCTION kdTreeKNN(root, query_point, k):
    best_neighbors ‚Üê PriorityQueue(max_size=k)  # Max heap by distance
    
    FUNCTION searchNode(node, depth):
        IF node == NULL:
            RETURN
        
        # Calculate distance to current node
        distance ‚Üê euclideanDistance(query_point.descriptor, node.point.descriptor)
        
        # Add to best neighbors if within top k
        IF best_neighbors.size() < k:
            best_neighbors.push((distance, node.point))
        ELSE IF distance < best_neighbors.top().distance:
            best_neighbors.pop()
            best_neighbors.push((distance, node.point))
        
        # Determine which subtree to search first
        split_dim ‚Üê node.split_dim
        IF split_dim == -1:  # Leaf node
            RETURN
        
        diff ‚Üê query_point.descriptor[split_dim] - node.point.descriptor[split_dim]
        
        IF diff < 0:
            first_subtree ‚Üê node.left
            second_subtree ‚Üê node.right
        ELSE:
            first_subtree ‚Üê node.right
            second_subtree ‚Üê node.left
        
        # Search closer subtree first
        searchNode(first_subtree, depth + 1)
        
        # Check if we need to search other subtree
        IF best_neighbors.size() < k OR ABS(diff) < best_neighbors.top().distance:
            searchNode(second_subtree, depth + 1)
    END FUNCTION
    
    searchNode(root, 0)
    
    # Extract sorted results
    results ‚Üê []
    WHILE NOT best_neighbors.isEmpty():
        results.prepend(best_neighbors.pop().point)
    
    RETURN results
END FUNCTION

‚ñ∂ OPTIMIZATIONS:
  ‚Ä¢ Use approximate nearest neighbors (FLANN, Annoy)
  ‚Ä¢ Hierarchical vocabulary tree for large-scale search
  ‚Ä¢ GPU acceleration for feature extraction
  ‚Ä¢ Inverted file index for billion-scale datasets
  ‚Ä¢ Product quantization for compact descriptors
  ‚Ä¢ Cascade filtering with cheap features first
```

---

## Game AI Pathfinding

### üéÆ Game Pathfinding (Jump Point Search)
**Purpose**: Ultra-fast pathfinding for video game characters
**Real Usage**: StarCraft II, Civilization VI, Path of Exile

```pseudocode
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë ALGORITHM: Jump Point Search (JPS) for Grid Pathfinding           ‚ïë
‚ïë Performance: 10-30x faster than A* | Memory: O(n) | Grid: 1000x1000‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñ∂ INPUT PARAMETERS:
  grid: Grid2D[Cell]                 # Game map grid
  start: GridPosition                # Starting position
  goal: GridPosition                 # Target position
  unit_size: int                     # Unit collision radius
  movement_type: MovementType        # Walk, fly, swim, etc.
  
‚ñ∂ OUTPUT:
  path: array[GridPosition]          # Optimal path
  waypoints: array[GridPosition]     # Smoothed path points
  cost: float                        # Total path cost

‚ñ∂ DATA STRUCTURES:
  Cell: {
    walkable: boolean
    cost: float                     # Movement cost (terrain type)
    height: float                   # Elevation
    flags: bitfield                 # water, lava, fog, etc.
  }
  
  JPSNode: {
    position: GridPosition
    g_cost: float
    h_cost: float
    f_cost: float
    parent: JPSNode
    forced_neighbors: array[Direction]
  }
  
  Direction: enum {
    NORTH, NORTHEAST, EAST, SOUTHEAST,
    SOUTH, SOUTHWEST, WEST, NORTHWEST
  }

‚ñ∂ ALGORITHM:
FUNCTION jumpPointSearch(grid, start, goal, unit_size, movement_type):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Preprocess Grid for Unit Size ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    IF unit_size > 1:
        # Create clearance map for larger units
        clearance_grid ‚Üê computeClearanceMap(grid, unit_size)
        USE clearance_grid INSTEAD OF grid
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Initialize JPS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    open_list ‚Üê PriorityQueue()  # Min heap by f_cost
    closed_set ‚Üê Set()
    
    start_node ‚Üê JPSNode{
        position: start,
        g_cost: 0,
        h_cost: octileDistance(start, goal),
        f_cost: h_cost,
        parent: NULL,
        forced_neighbors: []
    }
    
    open_list.push(start_node)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: Main JPS Loop ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    WHILE NOT open_list.isEmpty():
        current ‚Üê open_list.pop()
        
        IF current.position == goal:
            RETURN reconstructPath(current)
        
        closed_set.add(current.position)
        
        # Identify successors (jump points)
        successors ‚Üê identifySuccessors(current, grid, goal)
        
        FOR successor IN successors:
            IF successor.position IN closed_set:
                CONTINUE
            
            # Calculate jump cost
            jump_cost ‚Üê calculateJumpCost(
                current.position,
                successor.position,
                grid
            )
            
            tentative_g ‚Üê current.g_cost + jump_cost
            
            # Check if we found a better path
            existing ‚Üê findInOpenList(open_list, successor.position)
            IF existing != NULL:
                IF tentative_g >= existing.g_cost:
                    CONTINUE
                ELSE:
                    open_list.remove(existing)
            
            # Add successor to open list
            successor.g_cost ‚Üê tentative_g
            successor.h_cost ‚Üê octileDistance(successor.position, goal)
            successor.f_cost ‚Üê successor.g_cost + successor.h_cost
            successor.parent ‚Üê current
            
            open_list.push(successor)
    
    RETURN NULL  # No path found

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê FUNCTION: Identify Successors (Core JPS Logic) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    FUNCTION identifySuccessors(node, grid, goal):
        successors ‚Üê []
        neighbors ‚Üê []
        
        IF node.parent != NULL:
            # Get normalized direction from parent
            dx ‚Üê SIGN(node.position.x - node.parent.position.x)
            dy ‚Üê SIGN(node.position.y - node.parent.position.y)
            
            # Diagonal move
            IF dx != 0 AND dy != 0:
                # Check diagonal continuation
                IF isWalkable(grid, node.position.x + dx, node.position.y + dy):
                    neighbors.append(Direction(dx, dy))
                
                # Check horizontal component
                IF isWalkable(grid, node.position.x + dx, node.position.y):
                    neighbors.append(Direction(dx, 0))
                
                # Check vertical component
                IF isWalkable(grid, node.position.x, node.position.y + dy):
                    neighbors.append(Direction(0, dy))
                
                # Check for forced neighbors (diagonal case)
                IF NOT isWalkable(grid, node.position.x - dx, node.position.y) AND
                   isWalkable(grid, node.position.x - dx, node.position.y + dy):
                    neighbors.append(Direction(-dx, dy))
                
                IF NOT isWalkable(grid, node.position.x, node.position.y - dy) AND
                   isWalkable(grid, node.position.x + dx, node.position.y - dy):
                    neighbors.append(Direction(dx, -dy))
            
            # Straight move (horizontal or vertical)
            ELSE:
                IF dx == 0:  # Moving vertically
                    IF isWalkable(grid, node.position.x, node.position.y + dy):
                        neighbors.append(Direction(0, dy))
                    
                    # Check for forced neighbors
                    IF NOT isWalkable(grid, node.position.x + 1, node.position.y) AND
                       isWalkable(grid, node.position.x + 1, node.position.y + dy):
                        neighbors.append(Direction(1, dy))
                    
                    IF NOT isWalkable(grid, node.position.x - 1, node.position.y) AND
                       isWalkable(grid, node.position.x - 1, node.position.y + dy):
                        neighbors.append(Direction(-1, dy))
                
                ELSE:  # Moving horizontally
                    IF isWalkable(grid, node.position.x + dx, node.position.y):
                        neighbors.append(Direction(dx, 0))
                    
                    # Check for forced neighbors
                    IF NOT isWalkable(grid, node.position.x, node.position.y + 1) AND
                       isWalkable(grid, node.position.x + dx, node.position.y + 1):
                        neighbors.append(Direction(dx, 1))
                    
                    IF NOT isWalkable(grid, node.position.x, node.position.y - 1) AND
                       isWalkable(grid, node.position.x + dx, node.position.y - 1):
                        neighbors.append(Direction(dx, -1))
        
        ELSE:
            # No parent - consider all neighbors
            FOR direction IN ALL_DIRECTIONS:
                IF isWalkable(grid, node.position.x + direction.dx, 
                            node.position.y + direction.dy):
                    neighbors.append(direction)
        
        # Try to jump in each neighbor direction
        FOR direction IN neighbors:
            jump_point ‚Üê jump(node.position, direction, grid, goal)
            
            IF jump_point != NULL:
                successors.append(JPSNode{
                    position: jump_point,
                    parent: node
                })
        
        RETURN successors
    END FUNCTION
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê FUNCTION: Jump (Find Jump Points) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    FUNCTION jump(position, direction, grid, goal):
        next_x ‚Üê position.x + direction.dx
        next_y ‚Üê position.y + direction.dy
        
        # Check if position is walkable
        IF NOT isWalkable(grid, next_x, next_y):
            RETURN NULL
        
        next_pos ‚Üê GridPosition(next_x, next_y)
        
        # Check if we reached the goal
        IF next_pos == goal:
            RETURN next_pos
        
        # Check for forced neighbors
        IF direction.dx != 0 AND direction.dy != 0:  # Diagonal
            # Check horizontal forced neighbor
            IF (NOT isWalkable(grid, next_x - direction.dx, next_y) AND
                isWalkable(grid, next_x - direction.dx, next_y + direction.dy)) OR
               (NOT isWalkable(grid, next_x, next_y - direction.dy) AND
                isWalkable(grid, next_x + direction.dx, next_y - direction.dy)):
                RETURN next_pos
            
            # Recursively jump horizontally and vertically
            IF jump(next_pos, Direction(direction.dx, 0), grid, goal) != NULL OR
               jump(next_pos, Direction(0, direction.dy), grid, goal) != NULL:
                RETURN next_pos
        
        ELSE:  # Straight
            IF direction.dx != 0:  # Horizontal
                IF (NOT isWalkable(grid, next_x, next_y + 1) AND
                    isWalkable(grid, next_x + direction.dx, next_y + 1)) OR
                   (NOT isWalkable(grid, next_x, next_y - 1) AND
                    isWalkable(grid, next_x + direction.dx, next_y - 1)):
                    RETURN next_pos
            
            ELSE:  # Vertical
                IF (NOT isWalkable(grid, next_x + 1, next_y) AND
                    isWalkable(grid, next_x + 1, next_y + direction.dy)) OR
                   (NOT isWalkable(grid, next_x - 1, next_y) AND
                    isWalkable(grid, next_x - 1, next_y + direction.dy)):
                    RETURN next_pos
        
        # Continue jumping
        RETURN jump(next_pos, direction, grid, goal)
    END FUNCTION
END FUNCTION

‚ñ∂ PATH SMOOTHING:
FUNCTION smoothPath(path, grid):
    IF length(path) <= 2:
        RETURN path
    
    waypoints ‚Üê [path[0]]
    current_index ‚Üê 0
    
    WHILE current_index < length(path) - 1:
        farthest_visible ‚Üê current_index + 1
        
        # Find farthest visible point
        FOR i FROM current_index + 2 TO length(path) - 1:
            IF hasLineOfSight(path[current_index], path[i], grid):
                farthest_visible ‚Üê i
            ELSE:
                BREAK
        
        waypoints.append(path[farthest_visible])
        current_index ‚Üê farthest_visible
    
    RETURN waypoints
END FUNCTION

FUNCTION hasLineOfSight(start, end, grid):
    # Bresenham's line algorithm
    x0, y0 ‚Üê start.x, start.y
    x1, y1 ‚Üê end.x, end.y
    
    dx ‚Üê ABS(x1 - x0)
    dy ‚Üê ABS(y1 - y0)
    sx ‚Üê 1 IF x0 < x1 ELSE -1
    sy ‚Üê 1 IF y0 < y1 ELSE -1
    err ‚Üê dx - dy
    
    WHILE TRUE:
        IF NOT isWalkable(grid, x0, y0):
            RETURN FALSE
        
        IF x0 == x1 AND y0 == y1:
            RETURN TRUE
        
        e2 ‚Üê 2 √ó err
        
        IF e2 > -dy:
            err ‚Üê err - dy
            x0 ‚Üê x0 + sx
        
        IF e2 < dx:
            err ‚Üê err + dx
            y0 ‚Üê y0 + sy
END FUNCTION

‚ñ∂ OPTIMIZATIONS:
  ‚Ä¢ Preprocessing: Build jump point database offline
  ‚Ä¢ Hierarchical pathfinding for large maps
  ‚Ä¢ Goal bounding: Prune nodes outside goal direction
  ‚Ä¢ Parallel search for multiple units
  ‚Ä¢ Path caching for common routes
  ‚Ä¢ Incremental replanning for dynamic obstacles
```

---

## File System Search

### üìÅ File Content Search (Boyer-Moore String Matching)
**Purpose**: Fast text search in files and documents
**Real Usage**: grep, Windows Search, IDE find-in-files

```pseudocode
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë ALGORITHM: Boyer-Moore with Wildcards and Regex Support           ‚ïë
‚ïë Performance: O(n/m) average | Files: Millions | Speed: GB/s        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñ∂ INPUT PARAMETERS:
  pattern: string                    # Search pattern (may include wildcards)
  directory: DirectoryPath           # Root directory to search
  options: SearchOptions             # Case sensitivity, regex, etc.
  file_filters: array[string]        # File extensions to search
  
‚ñ∂ OUTPUT:
  matches: array[FileMatch]          # Files containing matches
  line_matches: array[LineMatch]     # Specific line matches
  statistics: SearchStats            # Files searched, time taken

‚ñ∂ DATA STRUCTURES:
  FileMatch: {
    file_path: string
    matches: array[LineMatch]
    file_size: long
    last_modified: datetime
  }
  
  LineMatch: {
    line_number: int
    column_start: int
    column_end: int
    line_text: string
    context_before: array[string]    # Previous lines for context
    context_after: array[string]     # Following lines for context
  }

‚ñ∂ ALGORITHM PART 1: Boyer-Moore Preprocessing
FUNCTION preprocessPattern(pattern, case_sensitive):
    m ‚Üê length(pattern)
    
    IF NOT case_sensitive:
        pattern ‚Üê toLowerCase(pattern)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Build Bad Character Table ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    bad_char ‚Üê array[256]  # ASCII table
    
    # Initialize all characters to pattern length
    FOR i FROM 0 TO 255:
        bad_char[i] ‚Üê m
    
    # Set shift values for pattern characters
    FOR i FROM 0 TO m-2:
        char ‚Üê pattern[i]
        bad_char[char] ‚Üê m - i - 1
        
        IF NOT case_sensitive:
            # Handle both cases
            upper ‚Üê toUpperCase(char)
            lower ‚Üê toLowerCase(char)
            bad_char[upper] ‚Üê m - i - 1
            bad_char[lower] ‚Üê m - i - 1
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Build Good Suffix Table ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    good_suffix ‚Üê array[m]
    
    # Compute suffix lengths
    suffix ‚Üê computeSuffixArray(pattern)
    
    # Case 1: Pattern contains another occurrence of suffix
    FOR i FROM 0 TO m-1:
        good_suffix[i] ‚Üê m
    
    j ‚Üê 0
    FOR i FROM m-1 TO 0 STEP -1:
        IF suffix[i] == i + 1:  # Prefix of pattern
            WHILE j < m - i - 1:
                IF good_suffix[j] == m:
                    good_suffix[j] ‚Üê m - i - 1
                j += 1
    
    # Case 2: Part of suffix occurs at beginning
    FOR i FROM 0 TO m-2:
        good_suffix[m - suffix[i] - 1] ‚Üê m - i - 1
    
    RETURN bad_char, good_suffix
END FUNCTION

‚ñ∂ ALGORITHM PART 2: File System Traversal
FUNCTION searchFiles(pattern, directory, options, file_filters):
    
    # Preprocess pattern
    IF options.use_regex:
        regex ‚Üê compileRegex(pattern, options)
    ELSE:
        bad_char, good_suffix ‚Üê preprocessPattern(pattern, options.case_sensitive)
    
    matches ‚Üê []
    stats ‚Üê SearchStats{files_searched: 0, bytes_processed: 0, start_time: NOW()}
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Parallel File Search ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    file_queue ‚Üê Queue()
    result_queue ‚Üê Queue()
    
    # Start worker threads
    num_workers ‚Üê getNumCPUs()
    workers ‚Üê []
    
    FOR i FROM 0 TO num_workers-1:
        worker ‚Üê Thread(searchWorker, file_queue, result_queue, pattern, options)
        worker.start()
        workers.append(worker)
    
    # Traverse directory tree
    traverseDirectory(directory, file_queue, file_filters, options)
    
    # Signal workers to stop
    FOR i FROM 0 TO num_workers-1:
        file_queue.put(NULL)
    
    # Collect results
    active_workers ‚Üê num_workers
    WHILE active_workers > 0:
        result ‚Üê result_queue.get()
        
        IF result == NULL:
            active_workers -= 1
        ELSE:
            matches.append(result)
            stats.files_searched += 1
            stats.bytes_processed += result.file_size
    
    stats.end_time ‚Üê NOW()
    stats.duration ‚Üê stats.end_time - stats.start_time
    
    RETURN matches, stats
END FUNCTION

‚ñ∂ ALGORITHM PART 3: Boyer-Moore Search in File
FUNCTION searchInFile(file_path, pattern, bad_char, good_suffix, options):
    file_matches ‚Üê []
    
    # Memory-map file for efficiency
    file_content ‚Üê memoryMapFile(file_path)
    n ‚Üê length(file_content)
    m ‚Üê length(pattern)
    
    IF n < m:
        RETURN []
    
    # Convert to lowercase if case-insensitive
    IF NOT options.case_sensitive:
        search_content ‚Üê toLowerCase(file_content)
    ELSE:
        search_content ‚Üê file_content
    
    # Boyer-Moore search
    i ‚Üê m - 1  # Position in text
    
    WHILE i < n:
        j ‚Üê m - 1  # Position in pattern
        
        # Match from right to left
        WHILE j >= 0 AND search_content[i] == pattern[j]:
            i -= 1
            j -= 1
        
        IF j < 0:
            # Match found
            match_start ‚Üê i + 1
            match_end ‚Üê match_start + m
            
            # Extract line context
            line_start ‚Üê findLineStart(file_content, match_start)
            line_end ‚Üê findLineEnd(file_content, match_end)
            line_number ‚Üê countLines(file_content, 0, line_start) + 1
            
            # Get context lines
            context_before ‚Üê []
            context_after ‚Üê []
            
            IF options.context_lines > 0:
                context_before ‚Üê getPreviousLines(
                    file_content,
                    line_start,
                    options.context_lines
                )
                context_after ‚Üê getNextLines(
                    file_content,
                    line_end,
                    options.context_lines
                )
            
            file_matches.append(LineMatch{
                line_number: line_number,
                column_start: match_start - line_start,
                column_end: match_end - line_start,
                line_text: file_content[line_start:line_end],
                context_before: context_before,
                context_after: context_after
            })
            
            # Move to next potential match
            i += m + 1
        
        ELSE:
            # Mismatch - use Boyer-Moore shift rules
            bad_char_shift ‚Üê bad_char[search_content[i]]
            good_suffix_shift ‚Üê good_suffix[j]
            
            shift ‚Üê MAX(bad_char_shift, good_suffix_shift)
            i += shift
    
    IF length(file_matches) > 0:
        RETURN FileMatch{
            file_path: file_path,
            matches: file_matches,
            file_size: n,
            last_modified: getFileModTime(file_path)
        }
    
    RETURN NULL
END FUNCTION

‚ñ∂ ALGORITHM PART 4: Regex and Wildcard Support
FUNCTION searchWithRegex(file_path, regex, options):
    file_matches ‚Üê []
    
    # Read file line by line for regex
    file ‚Üê openFile(file_path)
    line_number ‚Üê 0
    
    WHILE line ‚Üê file.readLine():
        line_number += 1
        
        IF NOT options.case_sensitive:
            search_line ‚Üê toLowerCase(line)
        ELSE:
            search_line ‚Üê line
        
        # Find all regex matches in line
        matches ‚Üê regex.findAll(search_line)
        
        FOR match IN matches:
            # Get context if requested
            context_before ‚Üê []
            context_after ‚Üê []
            
            IF options.context_lines > 0:
                # Rewind file to get previous lines
                saved_position ‚Üê file.tell()
                context_before ‚Üê getPreviousLinesFromFile(
                    file,
                    line_number,
                    options.context_lines
                )
                
                # Get following lines
                file.seek(saved_position)
                context_after ‚Üê getNextLinesFromFile(
                    file,
                    options.context_lines
                )
                file.seek(saved_position)
            
            file_matches.append(LineMatch{
                line_number: line_number,
                column_start: match.start,
                column_end: match.end,
                line_text: line,
                context_before: context_before,
                context_after: context_after
            })
    
    file.close()
    
    IF length(file_matches) > 0:
        RETURN FileMatch{
            file_path: file_path,
            matches: file_matches,
            file_size: getFileSize(file_path),
            last_modified: getFileModTime(file_path)
        }
    
    RETURN NULL
END FUNCTION

‚ñ∂ HELPER FUNCTIONS:
FUNCTION traverseDirectory(directory, file_queue, filters, options):
    # Iterative traversal with stack to avoid recursion limits
    dir_stack ‚Üê Stack()
    dir_stack.push(directory)
    
    WHILE NOT dir_stack.isEmpty():
        current_dir ‚Üê dir_stack.pop()
        
        TRY:
            entries ‚Üê listDirectory(current_dir)
            
            FOR entry IN entries:
                IF entry.is_directory:
                    IF NOT options.skip_hidden OR NOT entry.name.startsWith("."):
                        # Check if directory should be excluded
                        IF entry.name NOT IN options.exclude_dirs:
                            dir_stack.push(entry.path)
                
                ELSE:  # File
                    # Check file filters
                    IF matchesFilter(entry.name, filters):
                        IF NOT options.skip_binary OR NOT isBinary(entry.path):
                            file_queue.put(entry.path)
        
        CATCH PermissionError:
            # Skip directories we can't access
            CONTINUE
END FUNCTION

FUNCTION searchWorker(file_queue, result_queue, pattern, options):
    # Preprocess pattern for this thread
    IF options.use_regex:
        regex ‚Üê compileRegex(pattern, options)
    ELSE:
        bad_char, good_suffix ‚Üê preprocessPattern(pattern, options.case_sensitive)
    
    WHILE TRUE:
        file_path ‚Üê file_queue.get()
        
        IF file_path == NULL:
            result_queue.put(NULL)
            BREAK
        
        TRY:
            IF options.use_regex:
                result ‚Üê searchWithRegex(file_path, regex, options)
            ELSE:
                result ‚Üê searchInFile(file_path, pattern, bad_char, good_suffix, options)
            
            IF result != NULL:
                result_queue.put(result)
        
        CATCH Exception:
            # Skip files that can't be read
            CONTINUE
END FUNCTION

‚ñ∂ OPTIMIZATIONS:
  ‚Ä¢ SIMD instructions for parallel character comparison
  ‚Ä¢ Memory-mapped I/O for large files
  ‚Ä¢ Skip binary files using magic bytes
  ‚Ä¢ Index frequently searched directories
  ‚Ä¢ Bloom filters for quick negative matches
  ‚Ä¢ Parallel directory traversal
  ‚Ä¢ Incremental indexing with file watching
```

---

## Similarity Search

### üîé Locality-Sensitive Hashing for Similarity Search
**Purpose**: Finds similar items in high-dimensional spaces
**Real Usage**: Spotify song recommendations, YouTube video suggestions, Shazam

```pseudocode
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë ALGORITHM: LSH for Audio/Document Similarity Search               ‚ïë
‚ïë Dimensions: 1000+ | Dataset: 100M items | Accuracy: 90%           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñ∂ INPUT PARAMETERS:
  query_item: FeatureVector          # Query audio/document features
  database: array[Item]              # Collection of items
  lsh_index: LSHIndex               # Pre-built hash tables
  similarity_threshold: float        # Minimum similarity (0-1)
  num_results: int                   # Number of results to return
  
‚ñ∂ OUTPUT:
  similar_items: array[SimilarItem]  # Ranked similar items
  explanations: array[string]        # Why items are similar

‚ñ∂ DATA STRUCTURES:
  LSHIndex: {
    hash_tables: array[HashTable]    # Multiple hash tables
    hash_functions: array[array[HashFunction]]  # Functions per table
    num_tables: int                  # L parameter
    signature_length: int            # K parameter
  }
  
  HashTable: {
    buckets: HashMap[int, array[ItemID]]  # Hash -> item IDs
    function_family: string          # "minhash", "simhash", "p-stable"
  }
  
  AudioFeatures: {
    mfcc: array[float]              # Mel-frequency cepstral coefficients
    chroma: array[float]            # Pitch class profiles
    tempo: float
    spectral_centroid: array[float]
    zero_crossing_rate: float
  }

‚ñ∂ ALGORITHM PART 1: Build LSH Index (Offline)
FUNCTION buildLSHIndex(database, num_tables=20, signature_length=10):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Choose Hash Family Based on Similarity ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    sample_items ‚Üê randomSample(database, 100)
    feature_type ‚Üê detectFeatureType(sample_items)
    
    IF feature_type == "binary":
        hash_family ‚Üê "simhash"
    ELSE IF feature_type == "set":
        hash_family ‚Üê "minhash"
    ELSE:  # Continuous features
        hash_family ‚Üê "p-stable"
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Generate Hash Functions ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    lsh_index ‚Üê LSHIndex{
        hash_tables: [],
        hash_functions: [],
        num_tables: num_tables,
        signature_length: signature_length
    }
    
    FOR table_id FROM 0 TO num_tables-1:
        table_functions ‚Üê []
        
        FOR i FROM 0 TO signature_length-1:
            IF hash_family == "p-stable":
                # For Euclidean distance (L2)
                a ‚Üê randomGaussianVector(dimension)
                b ‚Üê randomUniform(0, width)
                w ‚Üê 4  # Bucket width
                
                hash_func ‚Üê FUNCTION(x):
                    RETURN FLOOR((dot(a, x) + b) / w)
                
            ELSE IF hash_family == "minhash":
                # For Jaccard similarity
                a ‚Üê randomInt(1, large_prime)
                b ‚Üê randomInt(0, large_prime)
                
                hash_func ‚Üê FUNCTION(x):
                    min_hash ‚Üê INFINITY
                    FOR element IN x:
                        hash_val ‚Üê (a √ó hash(element) + b) % large_prime
                        min_hash ‚Üê MIN(min_hash, hash_val)
                    RETURN min_hash
            
            ELSE IF hash_family == "simhash":
                # For cosine similarity
                random_hyperplane ‚Üê randomUnitVector(dimension)
                
                hash_func ‚Üê FUNCTION(x):
                    RETURN 1 IF dot(random_hyperplane, x) >= 0 ELSE 0
            
            table_functions.append(hash_func)
        
        lsh_index.hash_functions.append(table_functions)
        lsh_index.hash_tables.append(HashTable{buckets: HashMap()})
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: Hash All Items ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    FOR item IN database:
        FOR table_id FROM 0 TO num_tables-1:
            # Compute signature for this table
            signature ‚Üê []
            
            FOR hash_func IN lsh_index.hash_functions[table_id]:
                hash_value ‚Üê hash_func(item.features)
                signature.append(hash_value)
            
            # Combine signature into single hash
            bucket_key ‚Üê hashSignature(signature)
            
            # Add to hash table
            IF bucket_key NOT IN lsh_index.hash_tables[table_id].buckets:
                lsh_index.hash_tables[table_id].buckets[bucket_key] ‚Üê []
            
            lsh_index.hash_tables[table_id].buckets[bucket_key].append(item.id)
    
    RETURN lsh_index
END FUNCTION

‚ñ∂ ALGORITHM PART 2: Query Processing
FUNCTION findSimilarItems(query_item, lsh_index, database, threshold, num_results):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Generate Query Signatures ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    candidate_set ‚Üê Set()
    
    FOR table_id FROM 0 TO lsh_index.num_tables-1:
        # Compute query signature for this table
        signature ‚Üê []
        
        FOR hash_func IN lsh_index.hash_functions[table_id]:
            hash_value ‚Üê hash_func(query_item.features)
            signature.append(hash_value)
        
        bucket_key ‚Üê hashSignature(signature)
        
        # Get candidates from this bucket
        IF bucket_key IN lsh_index.hash_tables[table_id].buckets:
            candidates ‚Üê lsh_index.hash_tables[table_id].buckets[bucket_key]
            candidate_set.update(candidates)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Exact Distance Computation ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    similar_items ‚Üê []
    
    FOR candidate_id IN candidate_set:
        candidate ‚Üê database.getItem(candidate_id)
        
        # Compute exact similarity
        similarity ‚Üê computeSimilarity(query_item, candidate)
        
        IF similarity >= threshold:
            similar_items.append(SimilarItem{
                item: candidate,
                similarity: similarity,
                common_buckets: countCommonBuckets(query_item, candidate, lsh_index)
            })
    
    # Sort by similarity
    similar_items ‚Üê SORT(similar_items, key=similarity, descending=True)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: Multi-Probe LSH (Optional Enhancement) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    IF length(similar_items) < num_results:
        # Probe nearby buckets
        additional_candidates ‚Üê multiProbe(
            query_item,
            lsh_index,
            num_probes=10
        )
        
        FOR candidate_id IN additional_candidates:
            IF candidate_id NOT IN candidate_set:
                candidate ‚Üê database.getItem(candidate_id)
                similarity ‚Üê computeSimilarity(query_item, candidate)
                
                IF similarity >= threshold:
                    similar_items.append(SimilarItem{
                        item: candidate,
                        similarity: similarity
                    })
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 4: Generate Explanations ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    explanations ‚Üê []
    
    FOR item IN similar_items[:num_results]:
        explanation ‚Üê explainSimilarity(query_item, item.item)
        explanations.append(explanation)
    
    RETURN similar_items[:num_results], explanations
END FUNCTION

‚ñ∂ SPECIALIZED: Audio Fingerprinting (Shazam-style)
FUNCTION audioFingerprint(audio_signal, sample_rate=44100):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Spectrogram Generation ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Short-Time Fourier Transform
    window_size ‚Üê 4096
    hop_length ‚Üê window_size / 2
    spectrogram ‚Üê STFT(audio_signal, window_size, hop_length)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Find Constellation Points ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Find peaks in spectrogram
    peaks ‚Üê []
    
    FOR time_frame FROM 0 TO length(spectrogram[0])-1:
        FOR freq_bin FROM 0 TO length(spectrogram)-1:
            magnitude ‚Üê ABS(spectrogram[freq_bin][time_frame])
            
            # Check if local maximum
            is_peak ‚Üê TRUE
            FOR dt FROM -2 TO 2:
                FOR df FROM -2 TO 2:
                    IF dt == 0 AND df == 0:
                        CONTINUE
                    
                    neighbor_time ‚Üê time_frame + dt
                    neighbor_freq ‚Üê freq_bin + df
                    
                    IF inBounds(neighbor_time, neighbor_freq, spectrogram):
                        IF ABS(spectrogram[neighbor_freq][neighbor_time]) >= magnitude:
                            is_peak ‚Üê FALSE
                            BREAK
            
            IF is_peak AND magnitude > threshold:
                peaks.append({
                    time: time_frame √ó hop_length / sample_rate,
                    frequency: freq_bin √ó sample_rate / window_size,
                    magnitude: magnitude
                })
    
    # Keep only strongest peaks
    peaks ‚Üê SORT(peaks, key=magnitude, descending=True)[:200]
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: Generate Hashes from Peak Pairs ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    hashes ‚Üê []
    
    FOR i FROM 0 TO length(peaks)-1:
        anchor ‚Üê peaks[i]
        
        # Create target zone
        target_peaks ‚Üê []
        FOR j FROM i+1 TO MIN(i+20, length(peaks)-1):
            IF peaks[j].time - anchor.time > 0.2 AND
               peaks[j].time - anchor.time < 2.0:
                target_peaks.append(peaks[j])
        
        # Generate hashes from anchor-target pairs
        FOR target IN target_peaks[:3]:  # Limit fan-out
            hash_value ‚Üê packHash(
                anchor.frequency,
                target.frequency,
                target.time - anchor.time
            )
            
            hashes.append({
                hash: hash_value,
                time: anchor.time
            })
    
    RETURN hashes
END FUNCTION

‚ñ∂ HELPER FUNCTIONS:
FUNCTION computeSimilarity(item1, item2):
    IF item1.type == "audio":
        # Use acoustic similarity
        RETURN acousticSimilarity(item1.features, item2.features)
    
    ELSE IF item1.type == "document":
        # Use cosine similarity for text
        RETURN cosineSimilarity(item1.features, item2.features)
    
    ELSE IF item1.type == "image":
        # Use perceptual hash distance
        RETURN 1 - hammingDistance(item1.phash, item2.phash) / 64
    
    ELSE:
        # Default to Euclidean distance
        distance ‚Üê euclideanDistance(item1.features, item2.features)
        RETURN 1 / (1 + distance)  # Convert to similarity
END FUNCTION

FUNCTION explainSimilarity(query, result):
    explanations ‚Üê []
    
    IF query.type == "audio":
        IF ABS(query.tempo - result.tempo) < 5:
            explanations.append(f"Similar tempo (~{result.tempo} BPM)")
        
        IF cosineSimilarity(query.chroma, result.chroma) > 0.8:
            explanations.append("Similar harmonic content")
        
        IF correlate(query.mfcc, result.mfcc) > 0.7:
            explanations.append("Similar timbre/instrumentation")
    
    ELSE IF query.type == "document":
        common_terms ‚Üê intersection(query.top_terms, result.top_terms)
        IF length(common_terms) > 0:
            explanations.append(f"Common topics: {JOIN(common_terms[:3])}")
    
    RETURN JOIN(explanations, "; ")
END FUNCTION

‚ñ∂ OPTIMIZATIONS:
  ‚Ä¢ Use bit-packed signatures for memory efficiency
  ‚Ä¢ GPU acceleration for hash computation
  ‚Ä¢ Distributed hash tables for scale
  ‚Ä¢ Learned LSH functions using neural networks
  ‚Ä¢ Progressive refinement with multiple rounds
  ‚Ä¢ Cache popular queries
```

---

## Network Routing

### üåê Internet Routing (BGP Path Selection with Dijkstra)
**Purpose**: Routes data packets across the global internet
**Real Usage**: ISP routers, CDNs, cloud providers

```pseudocode
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë ALGORITHM: BGP Best Path Selection with QoS Constraints           ‚ïë
‚ïë Scale: 900K+ routes | Convergence: <30s | Updates: 1000/s         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñ∂ INPUT PARAMETERS:
  routing_table: RoutingTable        # Current BGP routes
  update: BGPUpdate                  # New route advertisement
  local_policies: array[Policy]      # Local routing policies
  network_topology: ASGraph          # Autonomous System graph
  qos_requirements: QoSConstraints   # Latency, bandwidth needs
  
‚ñ∂ OUTPUT:
  best_path: BGPRoute                # Selected route
  backup_paths: array[BGPRoute]      # Alternative routes
  actions: array[RouteAction]        # Install/withdraw actions

‚ñ∂ DATA STRUCTURES:
  BGPRoute: {
    prefix: IPPrefix                # Destination network
    next_hop: IPAddress             # Next router
    as_path: array[ASNumber]        # Path through ASes
    local_pref: int                 # Local preference (higher=better)
    med: int                        # Multi-exit discriminator
    origin: enum                    # IGP, EGP, or Incomplete
    communities: array[string]      # Route tags
    weight: int                     # Cisco-specific
  }
  
  ASGraph: {
    nodes: map[ASNumber, ASNode]
    edges: map[pair[AS,AS], ASLink]
  }
  
  ASLink: {
    latency: float                  # Milliseconds
    bandwidth: int                  # Mbps
    packet_loss: float              # Percentage
    cost: float                     # Financial cost
  }

‚ñ∂ ALGORITHM:
FUNCTION selectBestPath(routing_table, update, policies, topology, qos):
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 1: Validate and Filter Update ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    IF NOT validateBGPUpdate(update):
        RETURN NULL  # Invalid update
    
    # Check route filters
    FOR policy IN policies:
        IF policy.type == "IMPORT_FILTER":
            IF NOT policy.matches(update):
                RETURN NULL  # Filtered out
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 2: Find Competing Routes ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    prefix ‚Üê update.prefix
    existing_routes ‚Üê routing_table.getRoutes(prefix)
    candidate_routes ‚Üê existing_routes + [update]
    
    # Apply import policies (modify attributes)
    FOR route IN candidate_routes:
        FOR policy IN policies:
            IF policy.type == "ROUTE_MAP":
                applyRouteMap(route, policy)
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 3: BGP Best Path Algorithm ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Step-by-step elimination
    remaining ‚Üê candidate_routes.copy()
    
    # 1. Prefer highest WEIGHT (Cisco-specific)
    IF hasWeight(remaining[0]):
        max_weight ‚Üê MAX(route.weight FOR route IN remaining)
        remaining ‚Üê [r FOR r IN remaining IF r.weight == max_weight]
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 2. Prefer highest LOCAL_PREF
    max_local_pref ‚Üê MAX(route.local_pref FOR route IN remaining)
    remaining ‚Üê [r FOR r IN remaining IF r.local_pref == max_local_pref]
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 3. Prefer locally originated routes
    local_routes ‚Üê [r FOR r IN remaining IF r.next_hop == LOCAL]
    IF length(local_routes) > 0:
        remaining ‚Üê local_routes
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 4. Prefer shortest AS_PATH
    min_path_length ‚Üê MIN(length(route.as_path) FOR route IN remaining)
    remaining ‚Üê [r FOR r IN remaining IF length(r.as_path) == min_path_length]
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 5. Prefer lowest ORIGIN (IGP < EGP < Incomplete)
    origin_preference ‚Üê {"IGP": 0, "EGP": 1, "Incomplete": 2}
    min_origin ‚Üê MIN(origin_preference[route.origin] FOR route IN remaining)
    remaining ‚Üê [r FOR r IN remaining IF origin_preference[r.origin] == min_origin]
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 6. Prefer lowest MED (from same AS)
    grouped_by_as ‚Üê groupBy(remaining, lambda r: r.as_path[0])
    remaining_after_med ‚Üê []
    
    FOR as_num, routes IN grouped_by_as:
        min_med ‚Üê MIN(route.med FOR route IN routes)
        best_from_as ‚Üê [r FOR r IN routes IF r.med == min_med]
        remaining_after_med.extend(best_from_as)
    
    remaining ‚Üê remaining_after_med
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 7. Prefer eBGP over iBGP
    ebgp_routes ‚Üê [r FOR r IN remaining IF r.peer_type == "eBGP"]
    IF length(ebgp_routes) > 0:
        remaining ‚Üê ebgp_routes
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    # 8. Prefer path with lowest IGP metric to next hop
    igp_metrics ‚Üê {}
    FOR route IN remaining:
        igp_metrics[route] ‚Üê getIGPMetric(route.next_hop, topology)
    
    min_igp ‚Üê MIN(igp_metrics.values())
    remaining ‚Üê [r FOR r IN remaining IF igp_metrics[r] == min_igp]
    
    IF length(remaining) == 1:
        RETURN remaining[0]
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 4: Apply QoS Constraints ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    IF qos != NULL:
        qos_viable ‚Üê []
        
        FOR route IN remaining:
            path_metrics ‚Üê calculatePathMetrics(route, topology)
            
            IF path_metrics.latency <= qos.max_latency AND
               path_metrics.bandwidth >= qos.min_bandwidth AND
               path_metrics.packet_loss <= qos.max_packet_loss:
                qos_viable.append(route)
        
        IF length(qos_viable) > 0:
            remaining ‚Üê qos_viable
    
    # 9. Tiebreakers
    IF length(remaining) > 1:
        # Prefer oldest route (stability)
        oldest ‚Üê MIN(remaining, key=lambda r: r.received_time)
        RETURN oldest
    
    RETURN remaining[0]
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STEP 5: Find Backup Paths ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    FUNCTION findBackupPaths(best_path, all_routes, max_backups=2):
        backups ‚Üê []
        
        # Remove best path from candidates
        candidates ‚Üê [r FOR r IN all_routes IF r != best_path]
        
        FOR route IN candidates:
            # Check if disjoint enough
            as_overlap ‚Üê intersection(route.as_path, best_path.as_path)
            
            IF length(as_overlap) < 0.5 √ó length(best_path.as_path):
                backups.append(route)
                
                IF length(backups) >= max_backups:
                    BREAK
        
        RETURN backups
    END FUNCTION
END FUNCTION

‚ñ∂ PATH COMPUTATION WITH DIJKSTRA:
FUNCTION calculatePathMetrics(route, topology):
    # Use Dijkstra to find actual path metrics
    as_path ‚Üê route.as_path
    total_latency ‚Üê 0
    min_bandwidth ‚Üê INFINITY
    max_packet_loss ‚Üê 0
    
    FOR i FROM 0 TO length(as_path)-2:
        src_as ‚Üê as_path[i]
        dst_as ‚Üê as_path[i+1]
        
        link ‚Üê topology.getLink(src_as, dst_as)
        
        IF link != NULL:
            total_latency += link.latency
            min_bandwidth ‚Üê MIN(min_bandwidth, link.bandwidth)
            max_packet_loss ‚Üê MAX(max_packet_loss, link.packet_loss)
        ELSE:
            # Estimate if link data unavailable
            total_latency += estimateLatency(src_as, dst_as)
            min_bandwidth ‚Üê MIN(min_bandwidth, 1000)  # Default 1Gbps
    
    RETURN PathMetrics{
        latency: total_latency,
        bandwidth: min_bandwidth,
        packet_loss: max_packet_loss
    }
END FUNCTION

‚ñ∂ ROUTE INSTALLATION:
FUNCTION installRoute(best_path, routing_table, backup_paths):
    actions ‚Üê []
    
    # Check if replacing existing route
    existing ‚Üê routing_table.getRoute(best_path.prefix)
    
    IF existing != NULL:
        IF existing != best_path:
            actions.append(RouteAction{
                type: "WITHDRAW",
                route: existing
            })
    
    # Install new route
    actions.append(RouteAction{
        type: "INSTALL",
        route: best_path,
        priority: 0
    })
    
    # Install backup routes with lower priority
    FOR i, backup IN enumerate(backup_paths):
        actions.append(RouteAction{
            type: "INSTALL",
            route: backup,
            priority: i + 1
        })
    
    # Update FIB (Forwarding Information Base)
    FOR action IN actions:
        IF action.type == "INSTALL":
            routing_table.fib.insert(
                action.route.prefix,
                action.route.next_hop,
                action.priority
            )
        ELSE IF action.type == "WITHDRAW":
            routing_table.fib.remove(
                action.route.prefix,
                action.route.next_hop
            )
    
    RETURN actions
END FUNCTION

‚ñ∂ CONVERGENCE OPTIMIZATION:
FUNCTION optimizeBGPConvergence(routing_table, updates):
    # Batch processing for faster convergence
    updates_by_prefix ‚Üê groupBy(updates, lambda u: u.prefix)
    
    # Process withdrawals first (avoid loops)
    FOR prefix, prefix_updates IN updates_by_prefix:
        withdrawals ‚Üê [u FOR u IN prefix_updates IF u.type == "WITHDRAW"]
        FOR withdrawal IN withdrawals:
            routing_table.remove(withdrawal)
    
    # Then process announcements
    FOR prefix, prefix_updates IN updates_by_prefix:
        announcements ‚Üê [u FOR u IN prefix_updates IF u.type == "ANNOUNCE"]
        
        IF length(announcements) > 0:
            # Only run selection once per prefix
            best ‚Üê selectBestPath(routing_table, announcements, ...)
            installRoute(best, routing_table, ...)
    
    # MRAI timer (Min Route Advertisement Interval)
    scheduleTimer(30, sendUpdates)  # Batch outgoing updates
END FUNCTION

‚ñ∂ OPTIMIZATIONS:
  ‚Ä¢ Route aggregation to reduce table size
  ‚Ä¢ Incremental SPF for topology changes
  ‚Ä¢ Path caching for common destinations
  ‚Ä¢ Parallel route processing
  ‚Ä¢ Bloom filters for loop detection
  ‚Ä¢ Fast reroute with pre-computed backups
```

## Summary

These search algorithms power critical infrastructure across the internet and modern applications:

1. **Web Search**: PageRank + inverted indexes enable sub-second searches across billions of pages
2. **GPS Navigation**: A* with traffic data routes millions of drivers in real-time
3. **Databases**: B+ trees provide logarithmic search in tables with billions of rows
4. **Image Search**: KD-trees and SIFT features power visual search engines
5. **Game AI**: Jump Point Search enables real-time pathfinding for thousands of units
6. **File Search**: Boyer-Moore achieves GB/s search speeds in text files
7. **Similarity Search**: LSH finds similar items in high-dimensional spaces efficiently
8. **Network Routing**: BGP and Dijkstra route internet traffic globally

Each algorithm is optimized for its specific domain, demonstrating how search is fundamental to modern computing infrastructure.